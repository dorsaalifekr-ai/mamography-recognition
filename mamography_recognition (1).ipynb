{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7uBoefMut3C",
        "outputId": "b3b02a41-5623-4f19-b7d7-6a6521f7135a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n",
            "Python OK\n",
            "Torch: 2.8.0+cu126\n",
            "Torchvision: 0.23.0+cu126\n",
            "Sun Oct 19 09:11:07 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!python -V\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"Python OK\")\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "\n",
        "    print(\"Torch:\", torch.__version__)\n",
        "    print(\"Torchvision:\", torchvision.__version__)\n",
        "except Exception as e:\n",
        "    print(\"Torch import error:\", e)\n",
        "\n",
        "\n",
        "!nvidia-smi || echo \"NO_GPU\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install kaggle\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"➡️ لطفاً فایل kaggle.json را آپلود کنید…\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "if \"kaggle.json\" not in uploaded:\n",
        "    raise SystemExit(\"❌ kaggle.json آپلود نشد. دوباره اجرا و فایل را انتخاب کن.\")\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"wb\") as f:\n",
        "    f.write(uploaded[\"kaggle.json\"])\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!kaggle datasets list -s \"cbis-ddsm\" | head -n 5 || echo \"Kaggle auth failed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "hh3CKf3wvLEA",
        "outputId": "5af92fa4-2829-4889-e8f3-dfc28bf33197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➡️ لطفاً فایل kaggle.json را آپلود کنید…\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49592003-d082-4414-8248-d3eea4d6e421\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49592003-d082-4414-8248-d3eea4d6e421\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                     title                                                   size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------  -----------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "awsaf49/cbis-ddsm-breast-cancer-image-dataset           CBIS-DDSM: Breast Cancer Image Dataset            5318997088  2021-01-24 07:35:59.737000          45843        331  0.7058824        \n",
            "skooch/ddsm-mammography                                 DDSM Mammography                                  3093452937  2018-07-03 08:38:28.307000          10668        193  0.75             \n",
            "llkihn/ddsm-cbis-patch                                  DDSM CBIS Patch                                   1276311012  2021-01-02 23:19:16.743000            248          7  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/mammo_cad\"\n",
        "ZIP_DIR = f\"{BASE_DIR}/zips\"\n",
        "RAW_DIR = f\"{BASE_DIR}/raw\"\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(ZIP_DIR, exist_ok=True)\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"ZIP_DIR :\", ZIP_DIR)\n",
        "print(\"RAW_DIR :\", RAW_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg57glzevaAa",
        "outputId": "b8d01000-b4a1-4683-aa6b-1c1da3fb1e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BASE_DIR: /content/drive/MyDrive/mammo_cad\n",
            "ZIP_DIR : /content/drive/MyDrive/mammo_cad/zips\n",
            "RAW_DIR : /content/drive/MyDrive/mammo_cad/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "ZIP_DIR = \"/content/drive/MyDrive/mammo_cad/zips\"\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "\n",
        "zips_before = glob.glob(ZIP_DIR + \"/*.zip\")\n",
        "print(\"Zips already in Drive:\", len(zips_before))\n",
        "\n",
        "if len(zips_before) == 0:\n",
        "\n",
        "    !kaggle datasets download -d awsaf49/cbis-ddsm-breast-cancer-image-dataset -p \"$ZIP_DIR\"\n",
        "else:\n",
        "    print(\"⏭️ Zip already present, skipping download.\")\n",
        "\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "inner_any = glob.glob(RAW_DIR + \"/**/*\", recursive=True)\n",
        "print(\"Files already in RAW_DIR:\", len(inner_any))\n",
        "\n",
        "if len(inner_any) < 100:\n",
        "    for z in glob.glob(ZIP_DIR + \"/*.zip\"):\n",
        "        print(\"Extracting:\", os.path.basename(z))\n",
        "        with zipfile.ZipFile(z, \"r\") as zf:\n",
        "            zf.extractall(RAW_DIR)\n",
        "else:\n",
        "    print(\"⏭️ RAW_DIR already populated, skipping extract.\")\n",
        "\n",
        "import glob\n",
        "\n",
        "jpgs = glob.glob(f\"{RAW_DIR}/**/*.jpg\", recursive=True) + glob.glob(\n",
        "    f\"{RAW_DIR}/**/*.jpeg\", recursive=True\n",
        ")\n",
        "pngs = glob.glob(f\"{RAW_DIR}/**/*.png\", recursive=True)\n",
        "csvs = glob.glob(f\"{RAW_DIR}/**/*.csv\", recursive=True)\n",
        "\n",
        "print(\"Images -> JPG/JPEG:\", len(jpgs), \" PNG:\", len(pngs))\n",
        "print(\"CSV files:\", len(csvs))\n",
        "print(\"Sample image:\", (jpgs + pngs)[:2])\n",
        "print(\"Sample CSV:\", csvs[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7NMau-Vvvo1",
        "outputId": "c26f0514-f878-4c86-aaf1-bfb699f1fa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zips already in Drive: 0\n",
            "Dataset URL: https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset\n",
            "License(s): CC-BY-SA-3.0\n",
            "Downloading cbis-ddsm-breast-cancer-image-dataset.zip to /content/drive/MyDrive/mammo_cad/zips\n",
            "100% 4.95G/4.95G [00:34<00:00, 190MB/s]\n",
            "100% 4.95G/4.95G [00:34<00:00, 152MB/s]\n",
            "Files already in RAW_DIR: 0\n",
            "Extracting: cbis-ddsm-breast-cancer-image-dataset.zip\n",
            "Images -> JPG/JPEG: 10237  PNG: 0\n",
            "CSV files: 6\n",
            "Sample image: ['/content/drive/MyDrive/mammo_cad/raw/jpeg/1.3.6.1.4.1.9590.100.1.2.100018879311824535125115145152454291132/1-263.jpg', '/content/drive/MyDrive/mammo_cad/raw/jpeg/1.3.6.1.4.1.9590.100.1.2.100018879311824535125115145152454291132/2-241.jpg']\n",
            "Sample CSV: ['/content/drive/MyDrive/mammo_cad/raw/csv/calc_case_description_test_set.csv', '/content/drive/MyDrive/mammo_cad/raw/csv/calc_case_description_train_set.csv', '/content/drive/MyDrive/mammo_cad/raw/csv/dicom_info.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "UID_RE = re.compile(r\"(1(?:\\.\\d+){3,})\")\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = UID_RE.search(str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "# 3.1) Mass CSVs\n",
        "mass_csvs = glob.glob(f\"{RAW_DIR}/csv/mass_case_description_*_set.csv\")\n",
        "print(\"Mass CSVs:\", mass_csvs)\n",
        "\n",
        "mass_uids = set()\n",
        "mass_pid_by_uid = {}\n",
        "for c in mass_csvs:\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        uid = extract_uid(row.get(\"image file path\", \"\"))\n",
        "        if uid:\n",
        "            mass_uids.add(uid)\n",
        "            pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "            if pid:\n",
        "                mass_pid_by_uid[uid] = pid\n",
        "\n",
        "print(\"Total Mass UIDs:\", len(mass_uids))\n",
        "print(\"Sample Mass UIDs:\", list(mass_uids)[:5])\n",
        "\n",
        "# 3.2) Calc CSVs (فقط برای اینکه کنار بگذاریم‌شان)\n",
        "calc_csvs = glob.glob(f\"{RAW_DIR}/csv/calc_case_description_*_set.csv\")\n",
        "calc_uids = set()\n",
        "for c in calc_csvs:\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        uid = extract_uid(row.get(\"image file path\", \"\"))\n",
        "        if uid:\n",
        "            calc_uids.add(uid)\n",
        "\n",
        "print(\"Total Calc UIDs:\", len(calc_uids))\n",
        "print(\"Overlap Mass∩Calc:\", len(mass_uids & calc_uids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzItyTJ3yjQ7",
        "outputId": "64f839b3-1cdf-4f60-d7a8-e448d2bbe7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mass CSVs: ['/content/drive/MyDrive/mammo_cad/raw/csv/mass_case_description_test_set.csv', '/content/drive/MyDrive/mammo_cad/raw/csv/mass_case_description_train_set.csv']\n",
            "Total Mass UIDs: 1592\n",
            "Sample Mass UIDs: ['1.3.6.1.4.1.9590.100.1.2.244860754512847405721118721642884345502', '1.3.6.1.4.1.9590.100.1.2.143651499712850251702614043240502055735', '1.3.6.1.4.1.9590.100.1.2.236750089410442340428255082793184632940', '1.3.6.1.4.1.9590.100.1.2.96699930111611146740318739371000200310', '1.3.6.1.4.1.9590.100.1.2.401772036413964151122239013710383266724']\n",
            "Total Calc UIDs: 1511\n",
            "Overlap Mass∩Calc: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "CSV_DIR = f\"{RAW_DIR}/csv\"\n",
        "SRC_IMG_DIR = f\"{RAW_DIR}/jpeg\"\n",
        "\n",
        "UID_RE = re.compile(r\"(1(?:\\.\\d+){3,})\")\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = UID_RE.search(str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "\n",
        "mass_uids, mass_pid_by_uid = set(), {}\n",
        "mass_csvs = glob.glob(f\"{CSV_DIR}/mass_case_description_*_set.csv\")\n",
        "for c in mass_csvs:\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "        if u:\n",
        "            mass_uids.add(u)\n",
        "            pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "            if pid:\n",
        "                mass_pid_by_uid[u] = pid\n",
        "\n",
        "\n",
        "calc_uids = set()\n",
        "calc_csvs = glob.glob(f\"{CSV_DIR}/calc_case_description_*_set.csv\")\n",
        "for c in calc_csvs:\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "        if u:\n",
        "            calc_uids.add(u)\n",
        "\n",
        "# --- 3) UIDهای موجود در مسیر تصاویر jpeg ---\n",
        "jpeg_files = (\n",
        "    glob.glob(f\"{SRC_IMG_DIR}/**/*.jpg\", recursive=True)\n",
        "    + glob.glob(f\"{SRC_IMG_DIR}/**/*.jpeg\", recursive=True)\n",
        "    + glob.glob(f\"{SRC_IMG_DIR}/**/*.png\", recursive=True)\n",
        ")\n",
        "jpeg_uids = {extract_uid(p) for p in jpeg_files if extract_uid(p)}\n",
        "\n",
        "print(\n",
        "    \"Counts -> mass_uids(csv):\",\n",
        "    len(mass_uids),\n",
        "    \"| calc_uids(csv):\",\n",
        "    len(calc_uids),\n",
        "    \"| jpeg_uids(files):\",\n",
        "    len(jpeg_uids),\n",
        ")\n",
        "print(\"Intersection(jpeg ∩ mass):\", len(jpeg_uids & mass_uids))\n",
        "print(\"Intersection(jpeg ∩ calc):\", len(jpeg_uids & calc_uids))\n",
        "\n",
        "\n",
        "# چند نمونه برای sanity-check\n",
        "def sample(s, k=3):\n",
        "    s = list(s)\n",
        "    return s[: min(k, len(s))]\n",
        "\n",
        "\n",
        "print(\"Sample mass UID (csv):\", sample(mass_uids))\n",
        "print(\"Sample jpeg UID (files):\", sample(jpeg_uids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15B1B8Jz4db2",
        "outputId": "b1245580-d834-4f01-f209-2c78fb33f701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts -> mass_uids(csv): 1592 | calc_uids(csv): 1511 | jpeg_uids(files): 6774\n",
            "Intersection(jpeg ∩ mass): 0\n",
            "Intersection(jpeg ∩ calc): 0\n",
            "Sample mass UID (csv): ['1.3.6.1.4.1.9590.100.1.2.244860754512847405721118721642884345502', '1.3.6.1.4.1.9590.100.1.2.143651499712850251702614043240502055735', '1.3.6.1.4.1.9590.100.1.2.236750089410442340428255082793184632940']\n",
            "Sample jpeg UID (files): ['1.3.6.1.4.1.9590.100.1.2.259575017012299394931844213051161127499', '1.3.6.1.4.1.9590.100.1.2.306231427610466354735096101303712317796', '1.3.6.1.4.1.9590.100.1.2.291970834411466767515548699420271393213']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import re\n",
        "\n",
        "# مرحله ۴-B: ساخت نگاشت از dicom_info.csv و ترجمهٔ UIDهای mass به jpeg\n",
        "import pandas as pd\n",
        "\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "CSV_DIR = f\"{RAW_DIR}/csv\"\n",
        "UID_RE = re.compile(r\"(1(?:\\.\\d+){3,})\")\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = UID_RE.search(str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "# 1) خواندن dicom_info و ساخت دیکشنری dicom→jpeg\n",
        "df_info = pd.read_csv(f\"{CSV_DIR}/dicom_info.csv\")\n",
        "dicom_uid = df_info[\"file_path\"].apply(extract_uid)\n",
        "jpeg_uid = df_info[\"image_path\"].apply(extract_uid)\n",
        "dicom2jpeg = {d: j for d, j in zip(dicom_uid, jpeg_uid) if pd.notna(d) and pd.notna(j)}\n",
        "print(\"dicom2jpeg size:\", len(dicom2jpeg))\n",
        "\n",
        "# 2) اگر mass_uids/calc_uids از بخش A در حافظه هست از همان‌ها استفاده می‌کنیم؛\n",
        "#    اگر نبود دوباره سریع می‌سازیم.\n",
        "import os\n",
        "\n",
        "if \"mass_uids\" not in globals() or \"calc_uids\" not in globals():\n",
        "    mass_uids, mass_pid_by_uid = set(), {}\n",
        "    for c in glob.glob(f\"{CSV_DIR}/mass_case_description_*_set.csv\"):\n",
        "        df = pd.read_csv(c)\n",
        "        for _, row in df.iterrows():\n",
        "            u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "            if u:\n",
        "                mass_uids.add(u)\n",
        "                pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "                if pid:\n",
        "                    mass_pid_by_uid[u] = pid\n",
        "    calc_uids = set()\n",
        "    for c in glob.glob(f\"{CSV_DIR}/calc_case_description_*_set.csv\"):\n",
        "        df = pd.read_csv(c)\n",
        "        for _, row in df.iterrows():\n",
        "            u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "            if u:\n",
        "                calc_uids.add(u)\n",
        "\n",
        "# 3) ترجمهٔ UIDهای mass از فضای DICOM به فضای JPEG\n",
        "mass_uids_jpeg = {dicom2jpeg[u] for u in mass_uids if u in dicom2jpeg}\n",
        "print(\"mass_uids (dicom):\", len(mass_uids), \"→ mass_uids_jpeg:\", len(mass_uids_jpeg))\n",
        "\n",
        "# 4) هم‌پوشانی جدید با UIDهای فایل‌های jpeg\n",
        "import glob\n",
        "\n",
        "SRC_IMG_DIR = f\"{RAW_DIR}/jpeg\"\n",
        "jpeg_files = glob.glob(f\"{SRC_IMG_DIR}/**/*.jpg\", recursive=True) + glob.glob(\n",
        "    f\"{SRC_IMG_DIR}/**/*.jpeg\", recursive=True\n",
        ")\n",
        "jpeg_uids = {extract_uid(p) for p in jpeg_files if extract_uid(p)}\n",
        "print(\"jpeg_uids:\", len(jpeg_uids))\n",
        "print(\"Intersection(jpeg ∩ mass_jpeg):\", len(jpeg_uids & mass_uids_jpeg))\n",
        "\n",
        "# 5) چند نمونه برای sanity-check\n",
        "print(\"Sample mass_jpeg UID:\", list(mass_uids_jpeg)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBiwZBRa5izU",
        "outputId": "259090dc-b8d7-4782-d49c-c7d9ca234b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dicom2jpeg size: 6774\n",
            "mass_uids (dicom): 1592 → mass_uids_jpeg: 0\n",
            "jpeg_uids: 6774\n",
            "Intersection(jpeg ∩ mass_jpeg): 0\n",
            "Sample mass_jpeg UID: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import re\n",
        "\n",
        "# 4-C: پیدا کردن این‌که کدام UID در dicom_info با پوشه‌های jpeg/ یکی است\n",
        "import pandas as pd\n",
        "\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "CSV_DIR = f\"{RAW_DIR}/csv\"\n",
        "UID_RE = re.compile(r\"(1(?:\\.\\d+){3,})\")\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = UID_RE.search(str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "# 1) فایل dicom_info را بخوان\n",
        "info = pd.read_csv(f\"{CSV_DIR}/dicom_info.csv\")\n",
        "\n",
        "# 2) از ستون‌های مختلف، UID استخراج کن\n",
        "uid_from_file_path = info.get(\"file_path\", \"\").apply(extract_uid)\n",
        "uid_from_image_path = info.get(\"image_path\", \"\").apply(extract_uid)\n",
        "uid_series = info.get(\"SeriesInstanceUID\", \"\").apply(extract_uid)\n",
        "uid_study = info.get(\"StudyInstanceUID\", \"\").apply(extract_uid)\n",
        "\n",
        "set_file = set(x for x in uid_from_file_path.dropna().tolist() if isinstance(x, str))\n",
        "set_image = set(x for x in uid_from_image_path.dropna().tolist() if isinstance(x, str))\n",
        "set_series = set(x for x in uid_series.dropna().tolist() if isinstance(x, str))\n",
        "set_study = set(x for x in uid_study.dropna().tolist() if isinstance(x, str))\n",
        "\n",
        "print(\n",
        "    \"Counts in dicom_info -> file:\",\n",
        "    len(set_file),\n",
        "    \"| image:\",\n",
        "    len(set_image),\n",
        "    \"| series:\",\n",
        "    len(set_series),\n",
        "    \"| study:\",\n",
        "    len(set_study),\n",
        ")\n",
        "\n",
        "# 3) UIDهای پوشه‌های jpeg/ (از قبل)\n",
        "import glob\n",
        "\n",
        "jpeg_uids = set()\n",
        "for p in glob.glob(f\"{RAW_DIR}/jpeg/**/*.jpg\", recursive=True):\n",
        "    u = extract_uid(p)\n",
        "    if u:\n",
        "        jpeg_uids.add(u)\n",
        "\n",
        "print(\"jpeg_uids(files):\", len(jpeg_uids))\n",
        "\n",
        "# 4) هم‌پوشانی با هر ستون:\n",
        "print(\"Overlap jpeg∩file  :\", len(jpeg_uids & set_file))\n",
        "print(\"Overlap jpeg∩image :\", len(jpeg_uids & set_image))\n",
        "print(\"Overlap jpeg∩series:\", len(jpeg_uids & set_series))\n",
        "print(\"Overlap jpeg∩study :\", len(jpeg_uids & set_study))\n",
        "\n",
        "# 5) UIDهای mass را از هر دو ستونِ mass-CSV می‌گیریم (image file path و cropped image file path)\n",
        "mass_csvs = glob.glob(f\"{CSV_DIR}/mass_case_description_*_set.csv\")\n",
        "mass_uids_image = set()\n",
        "mass_uids_cropped = set()\n",
        "mass_pid_by_uid = {}\n",
        "for c in mass_csvs:\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        u1 = extract_uid(row.get(\"image file path\", \"\"))\n",
        "        u2 = extract_uid(row.get(\"cropped image file path\", \"\"))\n",
        "        if u1:\n",
        "            mass_uids_image.add(u1)\n",
        "            pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "            if pid:\n",
        "                mass_pid_by_uid[u1] = pid\n",
        "        if u2:\n",
        "            mass_uids_cropped.add(u2)\n",
        "            pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "            if pid and u2 not in mass_pid_by_uid:\n",
        "                mass_pid_by_uid[u2] = pid\n",
        "\n",
        "print(\"mass_uids_image  :\", len(mass_uids_image))\n",
        "print(\"mass_uids_cropped:\", len(mass_uids_cropped))\n",
        "\n",
        "# 6) ببینیم کدام‌یک از mass_uids_* با کدام ستون از dicom_info می‌نشیند\n",
        "print(\"Overlap mass_image ∩ file  :\", len(mass_uids_image & set_file))\n",
        "print(\"Overlap mass_image ∩ image :\", len(mass_uids_image & set_image))\n",
        "print(\"Overlap mass_image ∩ series:\", len(mass_uids_image & set_series))\n",
        "print(\"Overlap mass_image ∩ study :\", len(mass_uids_image & set_study))\n",
        "\n",
        "print(\"Overlap mass_crop  ∩ file  :\", len(mass_uids_cropped & set_file))\n",
        "print(\"Overlap mass_crop  ∩ image :\", len(mass_uids_cropped & set_image))\n",
        "print(\"Overlap mass_crop  ∩ series:\", len(mass_uids_cropped & set_series))\n",
        "print(\"Overlap mass_crop  ∩ study :\", len(mass_uids_cropped & set_study))\n",
        "\n",
        "\n",
        "# 7) چند UID نمونه از هر طرف برای sanity check\n",
        "def sample(s, k=3):\n",
        "    s = list(s)\n",
        "    return s[: min(k, len(s))]\n",
        "\n",
        "\n",
        "print(\"Sample jpeg_uids:\", sample(jpeg_uids))\n",
        "print(\"Sample set_image:\", sample(set_image))\n",
        "print(\"Sample set_series:\", sample(set_series))\n",
        "print(\"Sample mass_uids_image:\", sample(mass_uids_image))\n",
        "print(\"Sample mass_uids_cropped:\", sample(mass_uids_cropped))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dRMgGz7KTX",
        "outputId": "a0800aef-dbf6-4e9b-d80d-3a17b9d6d40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts in dicom_info -> file: 6774 | image: 6774 | series: 6774 | study: 6774\n",
            "jpeg_uids(files): 6774\n",
            "Overlap jpeg∩file  : 6774\n",
            "Overlap jpeg∩image : 6774\n",
            "Overlap jpeg∩series: 6774\n",
            "Overlap jpeg∩study : 0\n",
            "mass_uids_image  : 1592\n",
            "mass_uids_cropped: 1696\n",
            "Overlap mass_image ∩ file  : 0\n",
            "Overlap mass_image ∩ image : 0\n",
            "Overlap mass_image ∩ series: 0\n",
            "Overlap mass_image ∩ study : 1592\n",
            "Overlap mass_crop  ∩ file  : 0\n",
            "Overlap mass_crop  ∩ image : 0\n",
            "Overlap mass_crop  ∩ series: 0\n",
            "Overlap mass_crop  ∩ study : 1696\n",
            "Sample jpeg_uids: ['1.3.6.1.4.1.9590.100.1.2.259575017012299394931844213051161127499', '1.3.6.1.4.1.9590.100.1.2.306231427610466354735096101303712317796', '1.3.6.1.4.1.9590.100.1.2.291970834411466767515548699420271393213']\n",
            "Sample set_image: ['1.3.6.1.4.1.9590.100.1.2.306231427610466354735096101303712317796', '1.3.6.1.4.1.9590.100.1.2.259575017012299394931844213051161127499', '1.3.6.1.4.1.9590.100.1.2.291970834411466767515548699420271393213']\n",
            "Sample set_series: ['1.3.6.1.4.1.9590.100.1.2.306231427610466354735096101303712317796', '1.3.6.1.4.1.9590.100.1.2.259575017012299394931844213051161127499', '1.3.6.1.4.1.9590.100.1.2.291970834411466767515548699420271393213']\n",
            "Sample mass_uids_image: ['1.3.6.1.4.1.9590.100.1.2.244860754512847405721118721642884345502', '1.3.6.1.4.1.9590.100.1.2.143651499712850251702614043240502055735', '1.3.6.1.4.1.9590.100.1.2.236750089410442340428255082793184632940']\n",
            "Sample mass_uids_cropped: ['1.3.6.1.4.1.9590.100.1.2.128382012316476040430900783769217916', '1.3.6.1.4.1.9590.100.1.2.282915215111615665204682464000406265581', '1.3.6.1.4.1.9590.100.1.2.376315223111985762802667897893877252492']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import re\n",
        "\n",
        "# 4-D: ترجمه‌ی StudyInstanceUIDهای Mass به مجموعه‌ای از Image UIDها با dicom_info.csv\n",
        "import pandas as pd\n",
        "\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "CSV_DIR = f\"{RAW_DIR}/csv\"\n",
        "\n",
        "UID_RE = re.compile(r\"(1(?:\\.\\d+){3,})\")\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = UID_RE.search(str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "# 1) Mass studies + pid map از CSVهای رسمی\n",
        "mass_csvs = glob.glob(f\"{CSV_DIR}/mass_case_description_*_set.csv\")\n",
        "mass_study_uids = set()\n",
        "mass_pid_by_study = {}\n",
        "\n",
        "for c in mass_csvs:\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        # طبق یافته‌ی قبلی، 'image file path' و 'cropped image file path' حاوی StudyInstanceUID هستند\n",
        "        for col in [\"image file path\", \"cropped image file path\"]:\n",
        "            u = extract_uid(row.get(col, \"\"))\n",
        "            if u:\n",
        "                mass_study_uids.add(u)\n",
        "                pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "                if pid and u not in mass_pid_by_study:\n",
        "                    mass_pid_by_study[u] = pid\n",
        "\n",
        "print(\"Mass Study UIDs:\", len(mass_study_uids))\n",
        "\n",
        "# 2) dicom_info: نگاشت Study -> ImageUID و برعکس\n",
        "info = pd.read_csv(f\"{CSV_DIR}/dicom_info.csv\")\n",
        "study_uid_col = info.get(\"StudyInstanceUID\", \"\").apply(extract_uid)\n",
        "image_uid_col = info.get(\"image_path\", \"\").apply(extract_uid)\n",
        "\n",
        "study2images = {}\n",
        "image2study = {}\n",
        "for s_uid, i_uid in zip(study_uid_col, image_uid_col):\n",
        "    if isinstance(s_uid, str) and isinstance(i_uid, str):\n",
        "        study2images.setdefault(s_uid, set()).add(i_uid)\n",
        "        image2study[i_uid] = s_uid\n",
        "\n",
        "print(\"study2images keys:\", len(study2images), \"| image2study:\", len(image2study))\n",
        "\n",
        "# 3) تبدیل Mass Study → مجموعه‌ی Image UIDها در jpeg\n",
        "mass_uids_jpeg = set()\n",
        "for s in mass_study_uids:\n",
        "    if s in study2images:\n",
        "        mass_uids_jpeg.update(study2images[s])\n",
        "\n",
        "print(\"mass_uids_jpeg (translated):\", len(mass_uids_jpeg))\n",
        "\n",
        "# 4) هم‌پوشانی با UIDهای واقعی فایل‌های jpeg\n",
        "import glob\n",
        "\n",
        "jpeg_files = glob.glob(f\"{RAW_DIR}/jpeg/**/*.jpg\", recursive=True) + glob.glob(\n",
        "    f\"{RAW_DIR}/jpeg/**/*.jpeg\", recursive=True\n",
        ")\n",
        "jpeg_uids = {extract_uid(p) for p in jpeg_files if extract_uid(p)}\n",
        "\n",
        "print(\"jpeg_uids:\", len(jpeg_uids))\n",
        "print(\"Intersection(jpeg ∩ mass_uids_jpeg):\", len(jpeg_uids & mass_uids_jpeg))\n",
        "\n",
        "\n",
        "# 5) چند نمونه برای sanity check\n",
        "def sample(s, k=5):\n",
        "    s = list(s)\n",
        "    return s[: min(k, len(s))]\n",
        "\n",
        "\n",
        "print(\"Sample mass_study_uids:\", sample(mass_study_uids))\n",
        "print(\"Sample mass_uids_jpeg :\", sample(mass_uids_jpeg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVQ6YvlD8HIR",
        "outputId": "a5f1561d-8db3-4dc3-9ca9-a78df8fd7581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mass Study UIDs: 3288\n",
            "study2images keys: 6774 | image2study: 6774\n",
            "mass_uids_jpeg (translated): 3288\n",
            "jpeg_uids: 6774\n",
            "Intersection(jpeg ∩ mass_uids_jpeg): 3288\n",
            "Sample mass_study_uids: ['1.3.6.1.4.1.9590.100.1.2.244860754512847405721118721642884345502', '1.3.6.1.4.1.9590.100.1.2.143651499712850251702614043240502055735', '1.3.6.1.4.1.9590.100.1.2.236750089410442340428255082793184632940', '1.3.6.1.4.1.9590.100.1.2.128382012316476040430900783769217916', '1.3.6.1.4.1.9590.100.1.2.282915215111615665204682464000406265581']\n",
            "Sample mass_uids_jpeg : ['1.3.6.1.4.1.9590.100.1.2.165978598313546365331998685770369073828', '1.3.6.1.4.1.9590.100.1.2.285317105313152046222309992052850234643', '1.3.6.1.4.1.9590.100.1.2.306231427610466354735096101303712317796', '1.3.6.1.4.1.9590.100.1.2.345919480011577581036244773132668941332', '1.3.6.1.4.1.9590.100.1.2.305524546111378534235330467170895252659']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import hashlib\n",
        "\n",
        "# مرحله ۴-E: build patient-level split & copy\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "RAW_DIR = \"/content/drive/MyDrive/mammo_cad/raw\"\n",
        "CSV_DIR = f\"{RAW_DIR}/csv\"\n",
        "SRC_IMG_DIR = f\"{RAW_DIR}/jpeg\"\n",
        "DEST_ROOT = \"/content/data\"\n",
        "\n",
        "UID_RE = re.compile(r\"(1(?:\\.\\d+){3,})\")\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = UID_RE.search(str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "# 1) از بخش D: این‌ها باید در حافظه باشند:\n",
        "# - mass_uids_jpeg  (Image UIDهایی که Mass هستند)\n",
        "# - image2study     (Image UID -> Study UID)\n",
        "# - mass_pid_by_study (Study UID -> patient_id)\n",
        "assert \"mass_uids_jpeg\" in globals()\n",
        "assert \"image2study\" in globals()\n",
        "assert \"mass_pid_by_study\" in globals()\n",
        "\n",
        "# 2) ترجمه‌ی calc به Image UID برای حذف\n",
        "calc_study_uids = set()\n",
        "for c in glob.glob(f\"{CSV_DIR}/calc_case_description_*_set.csv\"):\n",
        "    df = pd.read_csv(c)\n",
        "    for _, row in df.iterrows():\n",
        "        u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "        if u:\n",
        "            calc_study_uids.add(u)\n",
        "\n",
        "# study2images را از image2study می‌سازیم\n",
        "from collections import defaultdict as dd\n",
        "\n",
        "study2images = dd(set)\n",
        "for img_uid, stu_uid in image2study.items():\n",
        "    if isinstance(stu_uid, str) and isinstance(img_uid, str):\n",
        "        study2images[stu_uid].add(img_uid)\n",
        "\n",
        "calc_uids_jpeg = set()\n",
        "for s in calc_study_uids:\n",
        "    if s in study2images:\n",
        "        calc_uids_jpeg.update(study2images[s])\n",
        "\n",
        "print(\n",
        "    \"calc_study_uids:\", len(calc_study_uids), \"-> calc_uids_jpeg:\", len(calc_uids_jpeg)\n",
        ")\n",
        "\n",
        "# 3) جمع‌آوری همه تصاویر و ساخت برچسب + pid\n",
        "all_imgs = glob.glob(f\"{SRC_IMG_DIR}/**/*.jpg\", recursive=True) + glob.glob(\n",
        "    f\"{SRC_IMG_DIR}/**/*.jpeg\", recursive=True\n",
        ")\n",
        "\n",
        "\n",
        "def uid_from_path(p):\n",
        "    m = UID_RE.search(p)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "def pid_from_path(p):\n",
        "    parent = pathlib.Path(p).parent.as_posix().lower()\n",
        "    return \"pid_\" + hashlib.md5(parent.encode()).hexdigest()[:10]\n",
        "\n",
        "\n",
        "by_patient = dd(list)\n",
        "\n",
        "for p in all_imgs:\n",
        "    img_uid = uid_from_path(p)\n",
        "    if not img_uid:\n",
        "        continue\n",
        "\n",
        "    if img_uid in calc_uids_jpeg:\n",
        "        continue\n",
        "\n",
        "    if img_uid in mass_uids_jpeg:\n",
        "        lab = \"mass\"\n",
        "        stu = image2study.get(img_uid, None)\n",
        "        if stu and stu in mass_pid_by_study:\n",
        "            pid = \"csv_\" + mass_pid_by_study[stu]\n",
        "        else:\n",
        "            pid = pid_from_path(p)\n",
        "    else:\n",
        "        lab = \"normal\"\n",
        "        pid = pid_from_path(p)\n",
        "    by_patient[pid].append((p, lab))\n",
        "\n",
        "print(\"Patients total:\", len(by_patient))\n",
        "\n",
        "# 4) split بیمار-محور 70/15/15\n",
        "patients = list(by_patient.keys())\n",
        "random.seed(42)\n",
        "random.shuffle(patients)\n",
        "n = len(patients)\n",
        "pt_train = set(patients[: int(0.7 * n)])\n",
        "pt_val = set(patients[int(0.7 * n) : int(0.85 * n)])\n",
        "pt_test = set(patients[int(0.85 * n) :])\n",
        "\n",
        "# 5) کپی با سقف حجم\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in [\"mass\", \"normal\"]:\n",
        "        d = f\"{DEST_ROOT}/{split}/{cls}\"\n",
        "        if os.path.exists(d):\n",
        "            shutil.rmtree(d, ignore_errors=True)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "CAP_PER_CLASS = 800  # اگر لازم بود 600/400 کن\n",
        "counters = {\n",
        "    (\"train\", \"mass\"): 0,\n",
        "    (\"train\", \"normal\"): 0,\n",
        "    (\"val\", \"mass\"): 0,\n",
        "    (\"val\", \"normal\"): 0,\n",
        "    (\"test\", \"mass\"): 0,\n",
        "    (\"test\", \"normal\"): 0,\n",
        "}\n",
        "\n",
        "\n",
        "def copy_group(pids, split):\n",
        "    for pid in pids:\n",
        "        for src, lab in by_patient[pid]:\n",
        "            key = (split, lab)\n",
        "            if counters[key] >= CAP_PER_CLASS:\n",
        "                continue\n",
        "            dst = f\"{DEST_ROOT}/{split}/{lab}/{pid}__{os.path.basename(src)}\"\n",
        "            shutil.copy2(src, dst)\n",
        "            counters[key] += 1\n",
        "\n",
        "\n",
        "copy_group(pt_train, \"train\")\n",
        "copy_group(pt_val, \"val\")\n",
        "copy_group(pt_test, \"test\")\n",
        "\n",
        "\n",
        "def count_in(folder):\n",
        "    return len(glob.glob(folder + \"/*\"))\n",
        "\n",
        "\n",
        "summary = {\n",
        "    \"train\": {\n",
        "        \"mass\": count_in(f\"{DEST_ROOT}/train/mass\"),\n",
        "        \"normal\": count_in(f\"{DEST_ROOT}/train/normal\"),\n",
        "    },\n",
        "    \"val\": {\n",
        "        \"mass\": count_in(f\"{DEST_ROOT}/val/mass\"),\n",
        "        \"normal\": count_in(f\"{DEST_ROOT}/val/normal\"),\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"mass\": count_in(f\"{DEST_ROOT}/test/mass\"),\n",
        "        \"normal\": count_in(f\"{DEST_ROOT}/test/normal\"),\n",
        "    },\n",
        "}\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpkA9PUH8_yJ",
        "outputId": "b79ba006-17e9-4f40-c560-12f3a2b1b283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc_study_uids: 1511 -> calc_uids_jpeg: 1511\n",
            "Patients total: 2867\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'mass': 799, 'normal': 800},\n",
              " 'val': {'mass': 759, 'normal': 550},\n",
              " 'test': {'mass': 797, 'normal': 576}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile /content/dataset.py\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MammogramFolders(Dataset):\n",
        "    \"\"\"\n",
        "    Expect: root/{mass,normal}/*.jpg|*.png\n",
        "    - تصاویر تک‌کاناله هستند؛ ولی برای مدل‌های ImageNet آن را به 3ch تبدیل می‌کنیم (در transforms).\n",
        "    - برچسب: normal=0, mass=1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        for label_name in [\"normal\", \"mass\"]:\n",
        "            label = 0 if label_name == \"normal\" else 1\n",
        "            for p in glob.glob(os.path.join(root, label_name, \"*\")):\n",
        "                if p.lower().endswith(\n",
        "                    (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")\n",
        "                ):\n",
        "                    self.samples.append((p, label))\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\n",
        "                f\"No images found under {root}. Expected 'mass' and 'normal' subfolders.\"\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)  # اینجا به 3ch تبدیل می‌کنیم\n",
        "        return img, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "    def pos_weight(self):\n",
        "        n_pos = sum(1 for _, y in self.samples if y == 1)\n",
        "        n_neg = sum(1 for _, y in self.samples if y == 0)\n",
        "        return (n_neg / max(1, n_pos)) if n_pos > 0 else 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em3__7D4AeZe",
        "outputId": "be6d9508-aa7e-4f24-9995-8973e880aaf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),  # gray → 3 کانال\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm)\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=4, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(ds_train), \"| pos_weight≈\", ds_train.pos_weight())\n",
        "\n",
        "x, y = next(iter(dl_train))\n",
        "print(\"Batch shape:\", x.shape, \"| labels:\", y)\n",
        "print(\"Min/Max:\", float(x.min()), float(x.max()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwlwVLAVBH4U",
        "outputId": "1a29156b-26c1-4eb3-9973-d4bbca433720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1599 | pos_weight≈ 1.0012515644555695\n",
            "Batch shape: torch.Size([4, 3, 512, 512]) | labels: tensor([0., 0., 0., 1.])\n",
            "Min/Max: 0.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class LiteAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    یک بلاک خیلی سبک برای توجه مکانی (spatial) + کانالی (channel) با محاسبه‌ی ارزان.\n",
        "    - کانالی: squeeze-excitation کوچک (r=8)\n",
        "    - مکانی: average + max pooling کانال‌ها و یک conv 7x7 کم‌هزینه\n",
        "    \"\"\"\n",
        "    def __init__(self, c, r=8):\n",
        "        super().__init__()\n",
        "        r = max(1, c // r)\n",
        "        # Channel attention (SE)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(c, r, kernel_size=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(r, c, kernel_size=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # Spatial attention\n",
        "        self.spatial = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel attention\n",
        "        w_c = self.se(x)\n",
        "        x = x * w_c\n",
        "        # Spatial attention\n",
        "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        s = torch.cat([avg_map, max_map], dim=1)\n",
        "        w_s = self.spatial(s)\n",
        "        return x * w_s\n",
        "\n",
        "class ResNet18LiteAtt(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        # backbone\n",
        "        self.backbone = resnet18(weights=\"IMAGENET1K_V1\" if pretrained else None)\n",
        "        # حذف avgpool و fc تا ویژگی‌ها را خودمان جمع‌بندی کنیم\n",
        "        self.features = nn.Sequential(\n",
        "            self.backbone.conv1,\n",
        "            self.backbone.bn1,\n",
        "            self.backbone.relu,\n",
        "            self.backbone.maxpool,\n",
        "\n",
        "            self.backbone.layer1,\n",
        "            self.backbone.layer2,\n",
        "            self.backbone.layer3,\n",
        "            self.backbone.layer4\n",
        "        )\n",
        "        c = 512  # خروجی resnet18\n",
        "        self.attn = LiteAttention(c, r=8)\n",
        "        # سرِ طبقه‌بندی: global pooling دوتایی + head کم‌پارامتر\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ورودی باید [B,3,H,W] باشد (ما قبلش به 3ch تبدیل می‌کنیم)\n",
        "        f = self.features(x)          # [B, 512, H/32, W/32]\n",
        "        f = self.attn(f)              # LiteAttention\n",
        "        out = self.head(f)            # [B, 1]\n",
        "        return out.squeeze(1)         # [B]\n",
        "\n",
        "def build_model(pretrained=True):\n",
        "    return ResNet18LiteAtt(pretrained=pretrained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB4Y09g0B0Yq",
        "outputId": "a2377ee4-2457-420c-ec26-c5da06dd99eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import build_model\n",
        "\n",
        "m = build_model(pretrained=True).cuda()\n",
        "x = torch.randn(2, 3, 512, 512).cuda()\n",
        "with torch.no_grad():\n",
        "    y = m(x)\n",
        "print(\"Model OK | logits shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIxD_OPXCRrk",
        "outputId": "643cd0eb-8675-42e3-c635-00e3b33cdd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 127MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model OK | logits shape: torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Supervised CNN training (Smoke Test: 2 epochs)\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#  (train/val/test)\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "print(\n",
        "    f\"Train size={len(ds_train)} | Val size={len(ds_val)} | Test size={len(ds_test)} | pos_weight≈{pos_weight.item():.3f}\"\n",
        ")\n",
        "\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "model = build_model(pretrained=True).to(DEVICE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # برای کنترل عدم‌توازن\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            psig = torch.sigmoid(logits)\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(psig.float().cpu().numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tf = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tf / (tf + fn + 1e-8)  # sensitivity/recall for positive class\n",
        "    spe = tn / (tn + fp + 1e-8)  # specificity\n",
        "\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "EPOCHS = 2\n",
        "best_auc = -1.0\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "ckpt_path = \"/content/runs/best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    n = 0\n",
        "    t0 = time.time()\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Transforms (train/val/test) ---\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "print(\n",
        "    f\"Train size={len(ds_train)} | Val size={len(ds_val)} | Test size={len(ds_test)} | pos_weight≈{pos_weight.item():.3f}\"\n",
        ")\n",
        "\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# --- Model / Loss / Optimizer ---\n",
        "model = build_model(pretrained=True).to(DEVICE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # برای کنترل عدم‌توازن\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            psig = torch.sigmoid(logits)\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(psig.float().cpu().numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tf = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tf / (tf + fn + 1e-8)  # sensitivity/recall for positive class\n",
        "    spe = tn / (tn + fp + 1e-8)  # specificity\n",
        "\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "# Train (2 epochs smoke test)\n",
        "EPOCHS = 2\n",
        "best_auc = -1.0\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "ckpt_path = \"/content/runs/best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    n = 0\n",
        "    t0 = time.time()\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45Bqz_wLDJre",
        "outputId": "d11194aa-2a07-47b7-f32e-3d6c0a11f578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size=1599 | Val size=1309 | Test size=1373 | pos_weight≈1.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3964383616.py:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size=1599 | Val size=1309 | Test size=1373 | pos_weight≈1.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3964383616.py:131: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "#  Supervised CNN training (Smoke Test: 2 epochs, AMP new API)\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# reproducibility سبک\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "\n",
        "print(\n",
        "    f\"Train size={len(ds_train)} | Val size={len(ds_val)} | Test size={len(ds_test)} | pos_weight≈{ds_train.pos_weight():.3f}\"\n",
        ")\n",
        "\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "#  Model / Loss / Optimizer\n",
        "model = build_model(pretrained=True).to(DEVICE)\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2)\n",
        "\n",
        "# ✅ AMP جدید\n",
        "from torch.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "\n",
        "scaler = GradScaler(device=DEVICE if DEVICE == \"cuda\" else \"cpu\")\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            psig = torch.sigmoid(logits)\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(psig.float().cpu().numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "# --- Train (2 epochs smoke test) ---\n",
        "EPOCHS = 2\n",
        "best_auc = -1.0\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "ckpt_path = \"/content/runs/best.pt\"\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    n = 0\n",
        "    t0 = time.time()\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running += loss.item() * x.size(0)\n",
        "        n += x.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss = running / max(1, n)\n",
        "    metrics_val = evaluate(model, dl_val)\n",
        "    print(\n",
        "        f\"[Epoch {epoch}/{EPOCHS}] loss={train_loss:.4f} | \"\n",
        "        f\"val: AUC={metrics_val['AUC']:.3f}, F1={metrics_val['F1']:.3f}, \"\n",
        "        f\"ACC={metrics_val['ACC']:.3f}, SEN={metrics_val['SEN']:.3f}, SPE={metrics_val['SPE']:.3f} \"\n",
        "        f\"| time={time.time()-t0:.1f}s\"\n",
        "    )\n",
        "\n",
        "    if metrics_val[\"AUC\"] > best_auc:\n",
        "        best_auc = metrics_val[\"AUC\"]\n",
        "        torch.save({\"model\": model.state_dict(), \"metrics_val\": metrics_val}, ckpt_path)\n",
        "        print(\"  ✅ Saved new best:\", ckpt_path)\n",
        "\n",
        "print(\"\\nDone. Best AUC on val:\", best_auc, \"| checkpoint:\", ckpt_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frM-xNjqHl2K",
        "outputId": "1ac3fdb7-91db-403a-8aff-da6c30bf664f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size=1599 | Val size=1309 | Test size=1373 | pos_weight≈1.001\n",
            "[Epoch 1/2] loss=0.4850 | val: AUC=0.856, F1=0.788, ACC=0.756, SEN=0.783, SPE=0.718 | time=382.0s\n",
            "  ✅ Saved new best: /content/runs/best.pt\n",
            "[Epoch 2/2] loss=0.3084 | val: AUC=0.955, F1=0.912, ACC=0.899, SEN=0.896, SPE=0.904 | time=381.7s\n",
            "  ✅ Saved new best: /content/runs/best.pt\n",
            "\n",
            "Done. Best AUC on val: 0.9549766439094501 | checkpoint: /content/runs/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate on TEST set using best checkpoint\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CKPT = \"/content/runs/best.pt\"\n",
        "\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "dl_test = DataLoader(\n",
        "    ds_test, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "model = build_model(pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE, weights_only=False)  # ⬅️ اصلاح اصلی\n",
        "model.load_state_dict(state[\"model\"])\n",
        "model.eval()\n",
        "\n",
        "ys, ps = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in dl_test:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        prob = torch.sigmoid(logits)\n",
        "        ys.append(y.float().cpu().numpy())\n",
        "        ps.append(prob.float().cpu().numpy())\n",
        "\n",
        "y_true = np.concatenate(ys)\n",
        "y_prob = np.concatenate(ps)\n",
        "y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "auc = roc_auc_score(y_true, y_prob)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "sen = tp / (tp + fn + 1e-8)\n",
        "spe = tn / (tn + fp + 1e-8)\n",
        "\n",
        "print(\n",
        "    f\"TEST -> AUC={auc:.3f} | F1={f1:.3f} | ACC={acc:.3f} | SEN={sen:.3f} | SPE={spe:.3f}\"\n",
        ")\n",
        "print(f\"Confusion Matrix counts: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q_WfIIqNppY",
        "outputId": "fe7e125d-e5af-4b10-b962-99892dea2caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a91a0504fe0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a91a0504fe0>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "\n",
            "    if w.is_alive():\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1647, in _shutdown_workers\n",
            "     if w.is_alive():  \n",
            "        ^ ^  ^^^^^^^^^^^^^^^^^^^^^\n",
            "^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            " \n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError^^: \n",
            "can only test a child processAssertionError: \n",
            "can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST -> AUC=0.946 | F1=0.891 | ACC=0.878 | SEN=0.861 | SPE=0.903\n",
            "Confusion Matrix counts: TN=520, FP=56, FN=111, TP=686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CKPT = \"/content/runs/best.pt\"\n",
        "SAVE_DIR = \"/content/gradcam\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 1) data + model\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "test_ds = MammogramFolders(\"/content/data/test\", transform=tfm)\n",
        "test_dl = DataLoader(\n",
        "    test_ds, batch_size=16, shuffle=False, num_workers=0\n",
        ")  # num_workers=0 برای جلوگیری از هشدارها\n",
        "\n",
        "model = build_model(pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE, weights_only=False)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "model.eval()\n",
        "\n",
        "# 2) پیش‌بینی همه‌ی تست برای پیدا کردن TP/FN\n",
        "all_probs, all_trues = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in test_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = torch.sigmoid(model(x))\n",
        "        all_probs.append(p.detach().cpu())\n",
        "        all_trues.append(y.detach().cpu())\n",
        "all_prob = torch.cat(all_probs).numpy()\n",
        "all_true = torch.cat(all_trues).numpy().astype(int)\n",
        "all_pred = (all_prob >= 0.5).astype(int).ravel()\n",
        "\n",
        "# اندیس‌های TP و FN\n",
        "tn, fp, fn, tp = confusion_matrix(all_true, all_pred).ravel()\n",
        "tp_idx = np.where((all_true == 1) & (all_pred == 1))[0][:3]\n",
        "fn_idx = np.where((all_true == 1) & (all_pred == 0))[0][:3]\n",
        "sel_idx = np.concatenate([tp_idx, fn_idx])\n",
        "\n",
        "# 3) پیاده‌سازی Grad-CAM روی لایه‌ی آخر ResNet18 (layer4)\n",
        "# پیدا کردن feature map از layer4\n",
        "target_module = model.features[-1]  # layer4\n",
        "feats = None\n",
        "grads = None\n",
        "\n",
        "\n",
        "def fwd_hook(m, inp, out):  # save feature maps\n",
        "    global feats\n",
        "    feats = out.detach()\n",
        "\n",
        "\n",
        "def bwd_hook(m, gin, gout):  # save grads\n",
        "    global grads\n",
        "    grads = gout[0].detach()\n",
        "\n",
        "\n",
        "h1 = target_module.register_forward_hook(fwd_hook)\n",
        "h2 = target_module.register_full_backward_hook(bwd_hook)\n",
        "\n",
        "to_img = transforms.ToPILImage()\n",
        "\n",
        "# برای دسترسی مستقیم به فایل‌ها (نام‌ها)\n",
        "test_paths = test_ds.samples\n",
        "\n",
        "\n",
        "def gradcam_for_index(i):\n",
        "    path, label = test_paths[i]\n",
        "    img0 = Image.open(path).convert(\"L\").resize((512, 512))\n",
        "    x = (\n",
        "        transforms.functional.to_tensor(img0).repeat(3, 1, 1).unsqueeze(0).to(DEVICE)\n",
        "    )  # 3ch\n",
        "    x.requires_grad_(True)\n",
        "\n",
        "    logits = model(x)  # [1]\n",
        "    score = torch.sigmoid(logits)[0]  # احتمالِ mass\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    score.backward()\n",
        "\n",
        "    # CAM: وزن‌ها = میانگین فضاییِ گرادیان‌ها در هر کانال\n",
        "    g = grads[0]  # [C,H,W]\n",
        "    w = g.mean(dim=(1, 2), keepdim=True)  # [C,1,1]\n",
        "    cam = (feats[0] * w).sum(0)  # [H,W]\n",
        "    cam = F.relu(cam)\n",
        "    cam = cam / (cam.max() + 1e-8)\n",
        "    cam_np = cam.detach().cpu().numpy()\n",
        "\n",
        "    # ترکیب heatmap با تصویر خاکستری\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(img0, cmap=\"gray\")\n",
        "    plt.imshow(\n",
        "        cam_np, alpha=0.35\n",
        "    )  # بدون انتخاب رنگ تا قانون رعایت شود (matplotlib default)\n",
        "    title = f\"prob={float(score):.2f} | true={int(label)} | pred={int(score>=0.5)}\"\n",
        "    plt.title(title)\n",
        "    out_path = os.path.join(SAVE_DIR, f\"gradcam_{i:05d}.png\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=160)\n",
        "    plt.close()\n",
        "    return out_path, title, path\n",
        "\n",
        "\n",
        "saved = []\n",
        "for i in sel_idx:\n",
        "    out_path, title, src = gradcam_for_index(int(i))\n",
        "    saved.append((out_path, title, src))\n",
        "\n",
        "h1.remove()\n",
        "h2.remove()\n",
        "\n",
        "print(\"Saved files:\")\n",
        "for p, t, s in saved:\n",
        "    print(\" -\", p, \" :: \", t, \" :: src=\", s)\n",
        "print(\"\\nTotal saved:\", len(saved), \"| Folder:\", SAVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXzlbS2LPVeo",
        "outputId": "fb39a533-fab7-4b54-9b1d-a437c7319530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2662472410.py:87: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  title = f\"prob={float(score):.2f} | true={int(label)} | pred={int(score>=0.5)}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved files:\n",
            " - /content/gradcam/gradcam_00576.png  ::  prob=1.00 | true=1 | pred=1  :: src= /content/data/test/mass/csv_p_01481__1-202.jpg\n",
            " - /content/gradcam/gradcam_00577.png  ::  prob=0.94 | true=1 | pred=1  :: src= /content/data/test/mass/csv_p_00915__1-289.jpg\n",
            " - /content/gradcam/gradcam_00578.png  ::  prob=0.78 | true=1 | pred=1  :: src= /content/data/test/mass/csv_p_00728__1-008.jpg\n",
            " - /content/gradcam/gradcam_00583.png  ::  prob=0.04 | true=1 | pred=0  :: src= /content/data/test/mass/csv_p_01697__2-164.jpg\n",
            " - /content/gradcam/gradcam_00591.png  ::  prob=0.22 | true=1 | pred=0  :: src= /content/data/test/mass/csv_p_00092__2-273.jpg\n",
            " - /content/gradcam/gradcam_00601.png  ::  prob=0.33 | true=1 | pred=0  :: src= /content/data/test/mass/csv_p_00881__1-219.jpg\n",
            "\n",
            "Total saved: 6 | Folder: /content/gradcam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ROC curve + Confusion Matrix figures for TEST split\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CKPT = \"/content/runs/best.pt\"\n",
        "\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_test = MammogramFolders(\"/content/data/test\", transform=tfm)\n",
        "dl_test = DataLoader(ds_test, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "model = build_model(pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE, weights_only=False)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "model.eval()\n",
        "\n",
        "ys, ps = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in dl_test:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = torch.sigmoid(model(x))\n",
        "        ys.append(y.cpu().numpy())\n",
        "        ps.append(p.cpu().numpy())\n",
        "y_true = np.concatenate(ys).astype(int)\n",
        "y_prob = np.concatenate(ps)\n",
        "\n",
        "# ROC\n",
        "fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC – Test\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/fig_roc_test.png\", dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# Confusion Matrix (threshold=0.5)\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(cm, interpolation=\"nearest\")\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "plt.title(\"Confusion Matrix – Test\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/fig_cm_test.png\", dpi=160)\n",
        "plt.close()\n",
        "\n",
        "print(\"Saved: /content/fig_roc_test.png\")\n",
        "print(\"Saved: /content/fig_cm_test.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWyfR08-RYyN",
        "outputId": "3443ec85-8854-444a-bdb5-97d8cae1703d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/fig_roc_test.png\n",
            "Saved: /content/fig_cm_test.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class LiteAttention(nn.Module):\n",
        "    def __init__(self, c, r=8):\n",
        "        super().__init__()\n",
        "        r = max(1, c // r)\n",
        "        # Channel attention (SE)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(c, r, kernel_size=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(r, c, kernel_size=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # Spatial attention\n",
        "        self.spatial = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # channel\n",
        "        w_c = self.se(x); x = x * w_c\n",
        "        # spatial\n",
        "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        w_s = self.spatial(torch.cat([avg_map, max_map], dim=1))\n",
        "        return x * w_s\n",
        "\n",
        "class ResNet18LiteAtt(nn.Module):\n",
        "    def __init__(self, pretrained=True, use_attn=True):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet18(weights=\"IMAGENET1K_V1\" if pretrained else None)\n",
        "        self.features = nn.Sequential(\n",
        "            self.backbone.conv1, self.backbone.bn1, self.backbone.relu, self.backbone.maxpool,\n",
        "            self.backbone.layer1, self.backbone.layer2, self.backbone.layer3, self.backbone.layer4\n",
        "        )\n",
        "        c = 512\n",
        "        self.use_attn = use_attn\n",
        "        self.attn = LiteAttention(c, r=8) if use_attn else nn.Identity()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        f = self.attn(f)\n",
        "        out = self.head(f)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "def build_model(pretrained=True, use_attn=True):\n",
        "    return ResNet18LiteAtt(pretrained=pretrained, use_attn=use_attn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dN_8QZAesG2",
        "outputId": "28a6212c-9de8-4e3f-b30b-fd1df0c6652e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import sys\n",
        "\n",
        "if \"model\" in sys.modules:\n",
        "    importlib.reload(sys.modules[\"model\"])\n",
        "else:\n",
        "    import model  # noqa\n",
        "\n",
        "from model import build_model\n",
        "\n",
        "print(\"build_model signature ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhdJDyUXirv1",
        "outputId": "9f8b5c5b-4237-4b9d-efbc-8c73158f40fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build_model signature ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  B2: Train Baseline (ResNet18 بدون Attention) و ارزیابی\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# reproducibility سبک\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# transforms\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# datasets/loaders\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_test = DataLoader(\n",
        "    ds_test, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# model/loss/opt\n",
        "model = build_model(pretrained=True, use_attn=False).to(DEVICE)  # ⬅️ بدون attention\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "scaler = GradScaler(device=DEVICE if DEVICE == \"cuda\" else \"cpu\")\n",
        "\n",
        "\n",
        "def eval_metrics(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE)\n",
        "            p = torch.sigmoid(model(x))\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(p.float().cpu().numpy())\n",
        "    import numpy as np\n",
        "\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "# train 5 epochs (quick ablation)\n",
        "EPOCHS = 5\n",
        "best_auc = -1\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "ckpt_noattn = \"/content/runs/baseline_noattn.pt\"\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    run = 0\n",
        "    n = 0\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        run += loss.item() * x.size(0)\n",
        "        n += x.size(0)\n",
        "    scheduler.step()\n",
        "    tr_loss = run / max(1, n)\n",
        "    mv = eval_metrics(model, dl_val)\n",
        "    print(\n",
        "        f\"[NoAttn Ep{ep}/{EPOCHS}] loss={tr_loss:.4f} | \"\n",
        "        f\"val: AUC={mv['AUC']:.3f}, F1={mv['F1']:.3f}, ACC={mv['ACC']:.3f}, \"\n",
        "        f\"SEN={mv['SEN']:.3f}, SPE={mv['SPE']:.3f} | {time.time()-t0:.1f}s\"\n",
        "    )\n",
        "    if mv[\"AUC\"] > best_auc:\n",
        "        best_auc = mv[\"AUC\"]\n",
        "        torch.save({\"model\": model.state_dict(), \"metrics_val\": mv}, ckpt_noattn)\n",
        "        print(\"  ✅ Saved best baseline:\", ckpt_noattn)\n",
        "\n",
        "# test metrics for baseline\n",
        "state = torch.load(ckpt_noattn, map_location=DEVICE, weights_only=False)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "model.eval()\n",
        "mb = eval_metrics(model, dl_test)\n",
        "print(\n",
        "    f\"\\nBASELINE (NoAttn) TEST -> AUC={mb['AUC']:.3f} | F1={mb['F1']:.3f} | \"\n",
        "    f\"ACC={mb['ACC']:.3f} | SEN={mb['SEN']:.3f} | SPE={mb['SPE']:.3f}\"\n",
        ")\n",
        "print(\"Checkpoint:\", ckpt_noattn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZplJDvAi4lf",
        "outputId": "892fc0e5-4b71-44ec-ace0-88378cb4b07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NoAttn Ep1/5] loss=0.4793 | val: AUC=0.909, F1=0.866, ACC=0.834, SEN=0.922, SPE=0.713 | 424.0s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep2/5] loss=0.3163 | val: AUC=0.949, F1=0.904, ACC=0.884, SEN=0.939, SPE=0.807 | 393.5s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep3/5] loss=0.2575 | val: AUC=0.961, F1=0.919, ACC=0.905, SEN=0.928, SPE=0.875 | 399.1s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep4/5] loss=0.2148 | val: AUC=0.966, F1=0.932, ACC=0.923, SEN=0.908, SPE=0.944 | 384.7s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep5/5] loss=0.1849 | val: AUC=0.971, F1=0.938, ACC=0.929, SEN=0.920, SPE=0.942 | 393.9s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "\n",
            "BASELINE (NoAttn) TEST -> AUC=0.965 | F1=0.922 | ACC=0.912 | SEN=0.896 | SPE=0.934\n",
            "Checkpoint: /content/runs/baseline_noattn.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# B3: Compare current checkpoints (unequal epochs)\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_ROOT = \"/content/data\"\n",
        "CKPT_ATTN = \"/content/runs/best.pt\"  # (+Attention, 2 epochs)\n",
        "CKPT_NOAT = \"/content/runs/baseline_noattn.pt\"  # (NoAttention, 5 epochs)\n",
        "\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm)\n",
        "dl_test = DataLoader(\n",
        "    ds_test, batch_size=64, shuffle=False, num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "def eval_ckpt(ckpt_path, use_attn):\n",
        "    m = build_model(pretrained=False, use_attn=use_attn).to(DEVICE)\n",
        "    st = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
        "    m.load_state_dict(st[\"model\"])\n",
        "    m.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dl_test:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "            p = torch.sigmoid(m(x))\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(p.float().cpu().numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    from sklearn.metrics import f1_score\n",
        "\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "m_attn = eval_ckpt(CKPT_ATTN, use_attn=True)\n",
        "m_noat = eval_ckpt(CKPT_NOAT, use_attn=False)\n",
        "\n",
        "print(\"(+Attention, 2ep) TEST:\", m_attn)\n",
        "print(\"(NoAttention, 5ep) TEST:\", m_noat)\n",
        "\n",
        "# بارچارت جمع‌وجور برای اسلاید\n",
        "metrics = [\"AUC\", \"F1\", \"ACC\", \"SEN\", \"SPE\"]\n",
        "attn_vals = [m_attn[k] for k in metrics]\n",
        "noat_vals = [m_noat[k] for k in metrics]\n",
        "\n",
        "plt.figure(figsize=(6.5, 3.0))\n",
        "x = np.arange(len(metrics))\n",
        "w = 0.35\n",
        "plt.bar(x - w / 2, attn_vals, width=w, label=\"+Attention (2ep)\")\n",
        "plt.bar(x + w / 2, noat_vals, width=w, label=\"Baseline (5ep)\")\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylim(0.6, 1.0)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Comparison on TEST (⚠️ different epochs)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/compare_now.png\", dpi=180)\n",
        "plt.close()\n",
        "print(\"Saved figure:\", \"/content/compare_now.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD-gnh_BstoS",
        "outputId": "e65f7c05-436e-40f1-90ab-f6a5192d6f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(+Attention, 2ep) TEST: {'AUC': np.float64(0.9462263000139411), 'F1': 0.8914879792072774, 'ACC': 0.8783685360524399, 'SEN': np.float64(0.8607277289728892), 'SPE': np.float64(0.9027777777621045)}\n",
            "(NoAttention, 5ep) TEST: {'AUC': np.float64(0.9651601491705004), 'F1': 0.921885087153002, 'ACC': 0.9118718135469774, 'SEN': np.float64(0.8958594730125989), 'SPE': np.float64(0.934027777761562)}\n",
            "Saved figure: /content/compare_now.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/mammo_cad\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# مسیرها را برای import اضافه کن\n",
        "for p in [\"/content\", BASE_DIR]:\n",
        "    if p not in sys.path:\n",
        "        sys.path.insert(0, p)\n",
        "\n",
        "# اگر فایل‌ها نیستند، بسازیم‌شان (آخرین نسخه با use_attn)\n",
        "if not os.path.exists(\"/content/dataset.py\"):\n",
        "    with open(\"/content/dataset.py\", \"w\") as f:\n",
        "        f.write(\n",
        "            r\"\"\"\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class MammogramFolders(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        for label_name in [\"normal\", \"mass\"]:\n",
        "            y = 0 if label_name==\"normal\" else 1\n",
        "            for p in glob.glob(os.path.join(root, label_name, \"*\")):\n",
        "                if p.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\")):\n",
        "                    self.samples.append((p, y))\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(f\"No images under {root}\")\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        p, y = self.samples[idx]\n",
        "        img = Image.open(p).convert(\"L\")\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, torch.tensor(y, dtype=torch.float32)\n",
        "    def pos_weight(self):\n",
        "        n_pos = sum(1 for _,y in self.samples if y==1)\n",
        "        n_neg = len(self.samples)-n_pos\n",
        "        return (n_neg / max(1,n_pos)) if n_pos>0 else 1.0\n",
        "\"\"\"\n",
        "        )\n",
        "    print(\"✅ wrote /content/dataset.py\")\n",
        "else:\n",
        "    print(\"ℹ️ dataset.py already exists at /content\")\n",
        "\n",
        "if not os.path.exists(\"/content/model.py\"):\n",
        "    with open(\"/content/model.py\", \"w\") as f:\n",
        "        f.write(\n",
        "            r\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class LiteAttention(nn.Module):\n",
        "    def __init__(self, c, r=8):\n",
        "        super().__init__()\n",
        "        r = max(1, c//r)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(c, r, 1, bias=False), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(r, c, 1, bias=False), nn.Sigmoid()\n",
        "        )\n",
        "        self.spatial = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, 7, padding=3, bias=False), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x * self.se(x)\n",
        "        avg = torch.mean(x, 1, keepdim=True)\n",
        "        mx, _ = torch.max(x, 1, keepdim=True)\n",
        "        attn = self.spatial(torch.cat([avg, mx], 1))\n",
        "        return x * attn\n",
        "\n",
        "class ResNet18LiteAtt(nn.Module):\n",
        "    def __init__(self, pretrained=True, use_attn=True):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet18(weights=\"IMAGENET1K_V1\" if pretrained else None)\n",
        "        self.features = nn.Sequential(\n",
        "            self.backbone.conv1, self.backbone.bn1, self.backbone.relu, self.backbone.maxpool,\n",
        "            self.backbone.layer1, self.backbone.layer2, self.backbone.layer3, self.backbone.layer4\n",
        "        )\n",
        "        c = 512\n",
        "        self.attn = LiteAttention(c) if use_attn else nn.Identity()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
        "            nn.Linear(c,128), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
        "            nn.Linear(128,1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        f = self.features(x); f = self.attn(f); out = self.head(f)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "def build_model(pretrained=True, use_attn=True):\n",
        "    return ResNet18LiteAtt(pretrained=pretrained, use_attn=use_attn)\n",
        "\"\"\"\n",
        "        )\n",
        "    print(\"✅ wrote /content/model.py\")\n",
        "else:\n",
        "    print(\"ℹ️ model.py already exists at /content\")\n",
        "\n",
        "# تست import\n",
        "try:\n",
        "    import importlib\n",
        "\n",
        "    import dataset\n",
        "    import model\n",
        "\n",
        "    importlib.reload(dataset)\n",
        "    importlib.reload(model)\n",
        "    from dataset import MammogramFolders\n",
        "    from model import build_model\n",
        "\n",
        "    print(\"✅ Import OK: MammogramFolders, build_model are available.\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Import failed:\", e)\n",
        "\n",
        "import shlex\n",
        "\n",
        "# نمایش کجا هستیم و چه فایل‌هایی داریم\n",
        "import subprocess\n",
        "\n",
        "print(\"\\nWorking dir:\", os.getcwd())\n",
        "print(\"Listing /content:\")\n",
        "print(\n",
        "    subprocess.run(\n",
        "        shlex.split(\"bash -lc 'ls -lah /content | sed -n \\\"1,80p\\\"'\"),\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    ).stdout\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52_UM-KwXHVz",
        "outputId": "a52e0d6a-d55b-4536-826a-d101cea897b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ wrote /content/dataset.py\n",
            "✅ wrote /content/model.py\n",
            "✅ Import OK: MammogramFolders, build_model are available.\n",
            "\n",
            "Working dir: /content\n",
            "Listing /content:\n",
            "total 32K\n",
            "drwxr-xr-x 1 root root 4.0K Oct 19 16:45 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct 19 16:00 ..\n",
            "drwxr-xr-x 4 root root 4.0K Oct 16 13:41 .config\n",
            "-rw-r--r-- 1 root root 1.1K Oct 19 16:45 dataset.py\n",
            "drwx------ 5 root root 4.0K Oct 19 16:45 drive\n",
            "-rw-r--r-- 1 root root 1.7K Oct 19 16:45 model.py\n",
            "drwxr-xr-x 2 root root 4.0K Oct 19 16:45 __pycache__\n",
            "drwxr-xr-x 1 root root 4.0K Oct 16 13:41 sample_data\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import hashlib\n",
        "\n",
        "# R2) Build or reuse patient-level split under Drive\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/mammo_cad\"\n",
        "RAW_DIR = f\"{BASE_DIR}/raw\"\n",
        "OUT_DIR = f\"{BASE_DIR}/data_split\"  # پایدار داخل Drive\n",
        "CAP_PER_CLASS = 800\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = re.search(r\"(1(?:\\.\\d+){3,})\", str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "def build_split(\n",
        "    raw_dir=RAW_DIR, out_dir=OUT_DIR, cap_per_class=CAP_PER_CLASS, seed=SEED\n",
        "):\n",
        "    csv_dir = os.path.join(raw_dir, \"csv\")\n",
        "    jpeg_dir = os.path.join(raw_dir, \"jpeg\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 1) Mass studies (CSV)\n",
        "    mass_csvs = glob.glob(os.path.join(csv_dir, \"mass_case_description_*_set.csv\"))\n",
        "    mass_study_uids, mass_pid_by_study = set(), {}\n",
        "    for c in mass_csvs:\n",
        "        df = pd.read_csv(c)\n",
        "        for _, row in df.iterrows():\n",
        "            for col in [\"image file path\", \"cropped image file path\"]:\n",
        "                u = extract_uid(row.get(col, \"\"))\n",
        "                if u:\n",
        "                    mass_study_uids.add(u)\n",
        "                    pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "                    if pid and u not in mass_pid_by_study:\n",
        "                        mass_pid_by_study[u] = pid\n",
        "\n",
        "    # 2) calc برای حذف\n",
        "    calc_study = set()\n",
        "    for c in glob.glob(os.path.join(csv_dir, \"calc_case_description_*_set.csv\")):\n",
        "        df = pd.read_csv(c)\n",
        "        for _, row in df.iterrows():\n",
        "            u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "            if u:\n",
        "                calc_study.add(u)\n",
        "\n",
        "    # 3) نگاشت Study↔Image از dicom_info.csv\n",
        "    info = pd.read_csv(os.path.join(csv_dir, \"dicom_info.csv\"))\n",
        "    study_col = info.get(\"StudyInstanceUID\", \"\").apply(extract_uid)\n",
        "    image_col = info.get(\"image_path\", \"\").apply(extract_uid)\n",
        "    study2images, image2study = defaultdict(set), {}\n",
        "    for s_uid, i_uid in zip(study_col, image_col):\n",
        "        if isinstance(s_uid, str) and isinstance(i_uid, str):\n",
        "            study2images[s_uid].add(i_uid)\n",
        "            image2study[i_uid] = s_uid\n",
        "\n",
        "    mass_uids_jpeg = set().union(\n",
        "        *[study2images[s] for s in mass_study_uids if s in study2images]\n",
        "    )\n",
        "    calc_uids_jpeg = set().union(\n",
        "        *[study2images[s] for s in calc_study if s in study2images]\n",
        "    )\n",
        "\n",
        "    # 4) گردآوری نمونه‌ها به‌صورت patient-centric\n",
        "    def pid_from_path(p):\n",
        "        parent = os.path.dirname(p).lower()\n",
        "        return \"pid_\" + hashlib.md5(parent.encode()).hexdigest()[:10]\n",
        "\n",
        "    by_patient = defaultdict(list)\n",
        "    imgs = glob.glob(os.path.join(jpeg_dir, \"**/*.jpg\"), recursive=True) + glob.glob(\n",
        "        os.path.join(jpeg_dir, \"**/*.jpeg\"), recursive=True\n",
        "    )\n",
        "    for p in imgs:\n",
        "        uid = extract_uid(p)\n",
        "        if not uid or uid in calc_uids_jpeg:\n",
        "            continue\n",
        "        if uid in mass_uids_jpeg:\n",
        "            lab = \"mass\"\n",
        "            stu = image2study.get(uid, None)\n",
        "            if stu and stu in mass_pid_by_study:\n",
        "                pid = \"csv_\" + mass_pid_by_study[stu]\n",
        "            else:\n",
        "                pid = pid_from_path(p)\n",
        "        else:\n",
        "            lab = \"normal\"\n",
        "            pid = pid_from_path(p)\n",
        "        by_patient[pid].append((p, lab))\n",
        "\n",
        "    # 5) split 70/15/15\n",
        "    patients = list(by_patient.keys())\n",
        "    random.seed(seed)\n",
        "    random.shuffle(patients)\n",
        "    n = len(patients)\n",
        "    train = set(patients[: int(0.7 * n)])\n",
        "    val = set(patients[int(0.7 * n) : int(0.85 * n)])\n",
        "    test = set(patients[int(0.85 * n) :])\n",
        "\n",
        "    # 6) ساخت دایرکتوری‌ها\n",
        "    for sp in [\"train\", \"val\", \"test\"]:\n",
        "        for c in [\"mass\", \"normal\"]:\n",
        "            os.makedirs(os.path.join(out_dir, sp, c), exist_ok=True)\n",
        "    counters = {\n",
        "        (\"train\", \"mass\"): 0,\n",
        "        (\"train\", \"normal\"): 0,\n",
        "        (\"val\", \"mass\"): 0,\n",
        "        (\"val\", \"normal\"): 0,\n",
        "        (\"test\", \"mass\"): 0,\n",
        "        (\"test\", \"normal\"): 0,\n",
        "    }\n",
        "\n",
        "    def copy_group(pids, split):\n",
        "        for pid in pids:\n",
        "            for src, lab in by_patient[pid]:\n",
        "                key = (split, lab)\n",
        "                if counters[key] >= cap_per_class:\n",
        "                    continue\n",
        "                dst = os.path.join(\n",
        "                    out_dir, split, lab, f\"{pid}__{os.path.basename(src)}\"\n",
        "                )\n",
        "                if not os.path.exists(dst):\n",
        "                    shutil.copy2(src, dst)\n",
        "                counters[key] += 1\n",
        "\n",
        "    copy_group(train, \"train\")\n",
        "    copy_group(val, \"val\")\n",
        "    copy_group(test, \"test\")\n",
        "\n",
        "    # 7) گزارش\n",
        "    def count(d):\n",
        "        return len(glob.glob(os.path.join(d, \"*\")))\n",
        "\n",
        "    summary = {\n",
        "        \"train\": {\n",
        "            \"mass\": count(os.path.join(out_dir, \"train\", \"mass\")),\n",
        "            \"normal\": count(os.path.join(out_dir, \"train\", \"normal\")),\n",
        "        },\n",
        "        \"val\": {\n",
        "            \"mass\": count(os.path.join(out_dir, \"val\", \"mass\")),\n",
        "            \"normal\": count(os.path.join(out_dir, \"val\", \"normal\")),\n",
        "        },\n",
        "        \"test\": {\n",
        "            \"mass\": count(os.path.join(out_dir, \"test\", \"mass\")),\n",
        "            \"normal\": count(os.path.join(out_dir, \"test\", \"normal\")),\n",
        "        },\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "\n",
        "# اگر قبلاً ساخته شده باشد، از همان استفاده می‌کنیم\n",
        "already = (\n",
        "    sum(\n",
        "        len(glob.glob(os.path.join(OUT_DIR, sp, c, \"*\")))\n",
        "        for sp in [\"train\", \"val\", \"test\"]\n",
        "        for c in [\"mass\", \"normal\"]\n",
        "    )\n",
        "    > 0\n",
        ")\n",
        "\n",
        "if already:\n",
        "    print(\"✅ data_split already exists in Drive:\", OUT_DIR)\n",
        "else:\n",
        "    print(\"⏳ Building patient-level split under:\", OUT_DIR)\n",
        "    summary = build_split()\n",
        "    print(\"✅ Built split. Summary:\", summary)\n",
        "\n",
        "# در هر صورت، یک لینک نمادین به /content/data برای کدهای بعدی می‌سازیم\n",
        "import os\n",
        "\n",
        "if os.path.islink(\"/content/data\") or os.path.exists(\"/content/data\"):\n",
        "    pass\n",
        "else:\n",
        "    os.symlink(OUT_DIR, \"/content/data\")\n",
        "print(\"DATA ready at:\", \"/content/data\")\n",
        "# شمارش نهایی\n",
        "final_counts = {\n",
        "    \"train\": {\n",
        "        \"mass\": len(glob.glob(\"/content/data/train/mass/*\")),\n",
        "        \"normal\": len(glob.glob(\"/content/data/train/normal/*\")),\n",
        "    },\n",
        "    \"val\": {\n",
        "        \"mass\": len(glob.glob(\"/content/data/val/mass/*\")),\n",
        "        \"normal\": len(glob.glob(\"/content/data/val/normal/*\")),\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"mass\": len(glob.glob(\"/content/data/test/mass/*\")),\n",
        "        \"normal\": len(glob.glob(\"/content/data/test/normal/*\")),\n",
        "    },\n",
        "}\n",
        "print(\"Final counts:\", final_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGE8peTHYxaU",
        "outputId": "5c3be2b2-c6ef-4d20-bc3d-7c91ed19c273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ data_split already exists in Drive: /content/drive/MyDrive/mammo_cad/data_split\n",
            "DATA ready at: /content/data\n",
            "Final counts: {'train': {'mass': 798, 'normal': 800}, 'val': {'mass': 265, 'normal': 241}, 'test': {'mass': 0, 'normal': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import hashlib\n",
        "\n",
        "# R3) Force rebuild patient-level split so that TEST is non-empty\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/mammo_cad\"\n",
        "RAW_DIR = f\"{BASE_DIR}/raw\"\n",
        "OUT_DIR = f\"{BASE_DIR}/data_split\"\n",
        "CAP_PER_CLASS = 800\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def extract_uid(s):\n",
        "    m = re.search(r\"(1(?:\\.\\d+){3,})\", str(s))\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "def rebuild_split(\n",
        "    raw_dir=RAW_DIR, out_dir=OUT_DIR, cap_per_class=CAP_PER_CLASS, seed=SEED\n",
        "):\n",
        "    # clean old split safely\n",
        "    for sp in [\"train\", \"val\", \"test\"]:\n",
        "        for c in [\"mass\", \"normal\"]:\n",
        "            d = os.path.join(out_dir, sp, c)\n",
        "            if os.path.isdir(d):\n",
        "                for f in glob.glob(os.path.join(d, \"*\")):\n",
        "                    try:\n",
        "                        os.remove(f)\n",
        "                    except:\n",
        "                        pass\n",
        "            else:\n",
        "                os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    csv_dir = os.path.join(raw_dir, \"csv\")\n",
        "    jpeg_dir = os.path.join(raw_dir, \"jpeg\")\n",
        "\n",
        "    # 1) Mass studies (CSV)\n",
        "    mass_csvs = glob.glob(os.path.join(csv_dir, \"mass_case_description_*_set.csv\"))\n",
        "    mass_study_uids, mass_pid_by_study = set(), {}\n",
        "    for c in mass_csvs:\n",
        "        df = pd.read_csv(c)\n",
        "        for _, row in df.iterrows():\n",
        "            for col in [\"image file path\", \"cropped image file path\"]:\n",
        "                u = extract_uid(row.get(col, \"\"))\n",
        "                if u:\n",
        "                    mass_study_uids.add(u)\n",
        "                    pid = str(row.get(\"patient_id\", \"\")).strip().lower()\n",
        "                    if pid and u not in mass_pid_by_study:\n",
        "                        mass_pid_by_study[u] = pid\n",
        "\n",
        "    # 2) calc برای حذف\n",
        "    calc_study = set()\n",
        "    for c in glob.glob(os.path.join(csv_dir, \"calc_case_description_*_set.csv\")):\n",
        "        df = pd.read_csv(c)\n",
        "        for _, row in df.iterrows():\n",
        "            u = extract_uid(row.get(\"image file path\", \"\"))\n",
        "            if u:\n",
        "                calc_study.add(u)\n",
        "\n",
        "    # 3) Study↔Image map\n",
        "    info = pd.read_csv(os.path.join(csv_dir, \"dicom_info.csv\"))\n",
        "    study_col = info.get(\"StudyInstanceUID\", \"\").apply(extract_uid)\n",
        "    image_col = info.get(\"image_path\", \"\").apply(extract_uid)\n",
        "    from collections import defaultdict\n",
        "\n",
        "    study2images, image2study = defaultdict(set), {}\n",
        "    for s_uid, i_uid in zip(study_col, image_col):\n",
        "        if isinstance(s_uid, str) and isinstance(i_uid, str):\n",
        "            study2images[s_uid].add(i_uid)\n",
        "            image2study[i_uid] = s_uid\n",
        "\n",
        "    mass_uids_jpeg = set().union(\n",
        "        *[study2images[s] for s in mass_study_uids if s in study2images]\n",
        "    )\n",
        "    calc_uids_jpeg = set().union(\n",
        "        *[study2images[s] for s in calc_study if s in study2images]\n",
        "    )\n",
        "\n",
        "    # 4) patient-centric grouping\n",
        "    def pid_from_path(p):\n",
        "        parent = os.path.dirname(p).lower()\n",
        "        return \"pid_\" + hashlib.md5(parent.encode()).hexdigest()[:10]\n",
        "\n",
        "    by_patient = defaultdict(list)\n",
        "    imgs = glob.glob(os.path.join(jpeg_dir, \"**/*.jpg\"), recursive=True) + glob.glob(\n",
        "        os.path.join(jpeg_dir, \"**/*.jpeg\"), recursive=True\n",
        "    )\n",
        "    for p in imgs:\n",
        "        uid = extract_uid(p)\n",
        "        if not uid or uid in calc_uids_jpeg:\n",
        "            continue\n",
        "        if uid in mass_uids_jpeg:\n",
        "            lab = \"mass\"\n",
        "            stu = image2study.get(uid, None)\n",
        "            if stu and stu in mass_pid_by_study:\n",
        "                pid = \"csv_\" + mass_pid_by_study[stu]\n",
        "            else:\n",
        "                pid = pid_from_path(p)\n",
        "        else:\n",
        "            lab = \"normal\"\n",
        "            pid = pid_from_path(p)\n",
        "        by_patient[pid].append((p, lab))\n",
        "\n",
        "    # 5) 70/15/15 split\n",
        "    patients = list(by_patient.keys())\n",
        "    random.seed(seed)\n",
        "    random.shuffle(patients)\n",
        "    n = len(patients)\n",
        "    train = set(patients[: int(0.7 * n)])\n",
        "    val = set(patients[int(0.7 * n) : int(0.85 * n)])\n",
        "    test = set(patients[int(0.85 * n) :])\n",
        "\n",
        "    # 6) copy with caps per class and per split\n",
        "    caps = {\n",
        "        (\"train\", \"mass\"): cap_per_class,\n",
        "        (\"train\", \"normal\"): cap_per_class,\n",
        "        (\"val\", \"mass\"): max(200, cap_per_class // 2),\n",
        "        (\"val\", \"normal\"): max(200, cap_per_class // 2),\n",
        "        (\"test\", \"mass\"): max(200, cap_per_class // 2),\n",
        "        (\"test\", \"normal\"): max(200, cap_per_class // 2),\n",
        "    }\n",
        "    counters = {k: 0 for k in caps}\n",
        "\n",
        "    def copy_group(pids, split):\n",
        "        for pid in pids:\n",
        "            for src, lab in by_patient[pid]:\n",
        "                key = (split, lab)\n",
        "                if counters[key] >= caps[key]:\n",
        "                    continue\n",
        "                dst = os.path.join(\n",
        "                    out_dir, split, lab, f\"{pid}__{os.path.basename(src)}\"\n",
        "                )\n",
        "                if not os.path.exists(dst):\n",
        "                    try:\n",
        "                        shutil.copy2(src, dst)\n",
        "                        counters[key] += 1\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "    copy_group(train, \"train\")\n",
        "    copy_group(val, \"val\")\n",
        "    copy_group(test, \"test\")\n",
        "\n",
        "    def count(d):\n",
        "        return len(glob.glob(os.path.join(d, \"*\")))\n",
        "\n",
        "    summary = {\n",
        "        \"train\": {\n",
        "            \"mass\": count(os.path.join(out_dir, \"train\", \"mass\")),\n",
        "            \"normal\": count(os.path.join(out_dir, \"train\", \"normal\")),\n",
        "        },\n",
        "        \"val\": {\n",
        "            \"mass\": count(os.path.join(out_dir, \"val\", \"mass\")),\n",
        "            \"normal\": count(os.path.join(out_dir, \"val\", \"normal\")),\n",
        "        },\n",
        "        \"test\": {\n",
        "            \"mass\": count(os.path.join(out_dir, \"test\", \"mass\")),\n",
        "            \"normal\": count(os.path.join(out_dir, \"test\", \"normal\")),\n",
        "        },\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "\n",
        "summary = rebuild_split()\n",
        "print(\"✅ Rebuilt split. Summary:\", summary)\n",
        "\n",
        "# link for /content/data\n",
        "if not (os.path.islink(\"/content/data\") or os.path.exists(\"/content/data\")):\n",
        "    os.symlink(OUT_DIR, \"/content/data\")\n",
        "print(\"DATA ready at:\", \"/content/data\")\n",
        "\n",
        "# final counts print again\n",
        "import glob\n",
        "\n",
        "final_counts = {\n",
        "    \"train\": {\n",
        "        \"mass\": len(glob.glob(\"/content/data/train/mass/*\")),\n",
        "        \"normal\": len(glob.glob(\"/content/data/train/normal/*\")),\n",
        "    },\n",
        "    \"val\": {\n",
        "        \"mass\": len(glob.glob(\"/content/data/val/mass/*\")),\n",
        "        \"normal\": len(glob.glob(\"/content/data/val/normal/*\")),\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"mass\": len(glob.glob(\"/content/data/test/mass/*\")),\n",
        "        \"normal\": len(glob.glob(\"/content/data/test/normal/*\")),\n",
        "    },\n",
        "}\n",
        "print(\"Final counts:\", final_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axr70DI1fAJj",
        "outputId": "075ab289-7d66-4ab4-c721-62b597f488f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rebuilt split. Summary: {'train': {'mass': 800, 'normal': 800}, 'val': {'mass': 400, 'normal': 400}, 'test': {'mass': 400, 'normal': 400}}\n",
            "DATA ready at: /content/data\n",
            "Final counts: {'train': {'mass': 800, 'normal': 800}, 'val': {'mass': 400, 'normal': 400}, 'test': {'mass': 400, 'normal': 400}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B4) Train +Attention for 5 epochs (same settings as baseline) and evaluate on TEST\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# reproducibility (سبک)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_test = DataLoader(\n",
        "    ds_test, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "model = build_model(pretrained=True, use_attn=True).to(DEVICE)\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "scaler = GradScaler(device=DEVICE if DEVICE == \"cuda\" else \"cpu\")\n",
        "\n",
        "\n",
        "def eval_metrics(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE)\n",
        "            p = torch.sigmoid(model(x))\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(p.float().cpu().numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "best_auc = -1\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "ckpt_attn5 = \"/content/runs/attn_5ep.pt\"\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    run = 0\n",
        "    n = 0\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        run += loss.item() * x.size(0)\n",
        "        n += x.size(0)\n",
        "    scheduler.step()\n",
        "    tr_loss = run / max(1, n)\n",
        "    mv = eval_metrics(model, dl_val)\n",
        "    print(\n",
        "        f\"[+Attn Ep{ep}/{EPOCHS}] loss={tr_loss:.4f} | \"\n",
        "        f\"val: AUC={mv['AUC']:.3f}, F1={mv['F1']:.3f}, ACC={mv['ACC']:.3f}, \"\n",
        "        f\"SEN={mv['SEN']:.3f}, SPE={mv['SPE']:.3f} | {time.time()-t0:.1f}s\"\n",
        "    )\n",
        "    if mv[\"AUC\"] > best_auc:\n",
        "        best_auc = mv[\"AUC\"]\n",
        "        torch.save({\"model\": model.state_dict(), \"metrics_val\": mv}, ckpt_attn5)\n",
        "        print(\"  ✅ Saved best +Attention:\", ckpt_attn5)\n",
        "\n",
        "# TEST با بهترین ایپاک\n",
        "state = torch.load(ckpt_attn5, map_location=DEVICE, weights_only=False)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "model.eval()\n",
        "mb = eval_metrics(model, dl_test)\n",
        "print(\n",
        "    f\"\\n+ATTENTION (5ep) TEST -> AUC={mb['AUC']:.3f} | F1={mb['F1']:.3f} | \"\n",
        "    f\"ACC={mb['ACC']:.3f} | SEN={mb['SEN']:.3f} | SPE={mb['SPE']:.3f}\"\n",
        ")\n",
        "print(\"Checkpoint:\", ckpt_attn5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H608zxJujD-3",
        "outputId": "4e181bc1-d334-4073-ace3-94c2864f3ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+Attn Ep1/5] loss=0.5073 | val: AUC=0.886, F1=0.773, ACC=0.792, SEN=0.707, SPE=0.877 | 158.0s\n",
            "  ✅ Saved best +Attention: /content/runs/attn_5ep.pt\n",
            "[+Attn Ep2/5] loss=0.3114 | val: AUC=0.949, F1=0.865, ACC=0.870, SEN=0.835, SPE=0.905 | 155.0s\n",
            "  ✅ Saved best +Attention: /content/runs/attn_5ep.pt\n",
            "[+Attn Ep3/5] loss=0.2520 | val: AUC=0.966, F1=0.919, ACC=0.917, SEN=0.932, SPE=0.902 | 156.5s\n",
            "  ✅ Saved best +Attention: /content/runs/attn_5ep.pt\n",
            "[+Attn Ep4/5] loss=0.2048 | val: AUC=0.971, F1=0.913, ACC=0.907, SEN=0.970, SPE=0.845 | 157.1s\n",
            "  ✅ Saved best +Attention: /content/runs/attn_5ep.pt\n",
            "[+Attn Ep5/5] loss=0.1911 | val: AUC=0.979, F1=0.944, ACC=0.944, SEN=0.955, SPE=0.932 | 156.9s\n",
            "  ✅ Saved best +Attention: /content/runs/attn_5ep.pt\n",
            "\n",
            "+ATTENTION (5ep) TEST -> AUC=0.973 | F1=0.925 | ACC=0.924 | SEN=0.937 | SPE=0.910\n",
            "Checkpoint: /content/runs/attn_5ep.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-train BASELINE (No Attention) for 5 epochs so we can compare again\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# reproducibility سبک\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# transforms\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm_eval)\n",
        "\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_test = DataLoader(\n",
        "    ds_test, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# model/loss/opt (⚠️ use_attn=False)\n",
        "model = build_model(pretrained=True, use_attn=False).to(DEVICE)\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "scaler = GradScaler(device=DEVICE if DEVICE == \"cuda\" else \"cpu\")\n",
        "\n",
        "\n",
        "def eval_metrics(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE)\n",
        "            p = torch.sigmoid(model(x))\n",
        "            ys.append(y.float().cpu().numpy())\n",
        "            ps.append(p.float().cpu().numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    return {\"AUC\": auc, \"F1\": f1, \"ACC\": acc, \"SEN\": sen, \"SPE\": spe}\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "best_auc = -1\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "ckpt_noattn = \"/content/runs/baseline_noattn.pt\"\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    run = 0\n",
        "    n = 0\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        run += loss.item() * x.size(0)\n",
        "        n += x.size(0)\n",
        "    scheduler.step()\n",
        "    tr_loss = run / max(1, n)\n",
        "    mv = eval_metrics(model, dl_val)\n",
        "    print(\n",
        "        f\"[NoAttn Ep{ep}/{EPOCHS}] loss={tr_loss:.4f} | \"\n",
        "        f\"val: AUC={mv['AUC']:.3f}, F1={mv['F1']:.3f}, ACC={mv['ACC']:.3f}, \"\n",
        "        f\"SEN={mv['SEN']:.3f}, SPE={mv['SPE']:.3f} | {time.time()-t0:.1f}s\"\n",
        "    )\n",
        "    if mv[\"AUC\"] > best_auc:\n",
        "        best_auc = mv[\"AUC\"]\n",
        "        torch.save({\"model\": model.state_dict(), \"metrics_val\": mv}, ckpt_noattn)\n",
        "        print(\"  ✅ Saved best baseline:\", ckpt_noattn)\n",
        "\n",
        "# TEST metrics for baseline\n",
        "state = torch.load(ckpt_noattn, map_location=DEVICE, weights_only=False)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "model.eval()\n",
        "mb = eval_metrics(model, dl_test)\n",
        "print(\n",
        "    f\"\\nBASELINE (NoAttn, 5ep) TEST -> AUC={mb['AUC']:.3f} | F1={mb['F1']:.3f} | \"\n",
        "    f\"ACC={mb['ACC']:.3f} | SEN={mb['SEN']:.3f} | SPE={mb['SPE']:.3f}\"\n",
        ")\n",
        "print(\"Checkpoint:\", ckpt_noattn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HhMXsWp0px",
        "outputId": "49423020-1992-44c0-df81-4e5564bee4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NoAttn Ep1/5] loss=0.4838 | val: AUC=0.932, F1=0.846, ACC=0.853, SEN=0.807, SPE=0.897 | 158.9s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep2/5] loss=0.3286 | val: AUC=0.962, F1=0.903, ACC=0.897, SEN=0.950, SPE=0.845 | 159.3s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep3/5] loss=0.2396 | val: AUC=0.969, F1=0.914, ACC=0.911, SEN=0.942, SPE=0.880 | 156.3s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep4/5] loss=0.2202 | val: AUC=0.975, F1=0.919, ACC=0.916, SEN=0.955, SPE=0.877 | 153.7s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "[NoAttn Ep5/5] loss=0.1864 | val: AUC=0.976, F1=0.934, ACC=0.934, SEN=0.937, SPE=0.930 | 159.2s\n",
            "  ✅ Saved best baseline: /content/runs/baseline_noattn.pt\n",
            "\n",
            "BASELINE (NoAttn, 5ep) TEST -> AUC=0.970 | F1=0.929 | ACC=0.930 | SEN=0.922 | SPE=0.937\n",
            "Checkpoint: /content/runs/baseline_noattn.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# B5) Fair comparison (both 5 epochs) on TEST\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_ROOT = \"/content/data\"\n",
        "CKPT_ATTN = \"/content/runs/attn_5ep.pt\"\n",
        "CKPT_NOAT = \"/content/runs/baseline_noattn.pt\"\n",
        "\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "dl_test = DataLoader(\n",
        "    MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm),\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "\n",
        "def eval_ckpt(path, use_attn):\n",
        "    m = build_model(pretrained=False, use_attn=use_attn).to(DEVICE)\n",
        "    st = torch.load(path, map_location=DEVICE, weights_only=False)\n",
        "    m.load_state_dict(st[\"model\"])\n",
        "    m.eval()\n",
        "    Ys, Ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dl_test:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "            p = torch.sigmoid(m(x))\n",
        "            Ys.append(y.float().cpu().numpy())\n",
        "            Ps.append(p.float().cpu().numpy())\n",
        "    y_true = np.concatenate(Ys)\n",
        "    y_prob = np.concatenate(Ps)\n",
        "    y_pred = (y_prob >= 0.5).astype(np.float32)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    return {\n",
        "        \"AUC\": float(auc),\n",
        "        \"F1\": float(f1),\n",
        "        \"ACC\": float(acc),\n",
        "        \"SEN\": float(sen),\n",
        "        \"SPE\": float(spe),\n",
        "    }\n",
        "\n",
        "\n",
        "m_attn = eval_ckpt(CKPT_ATTN, use_attn=True)\n",
        "m_noatt = eval_ckpt(CKPT_NOAT, use_attn=False)\n",
        "\n",
        "print(\"=== TEST (5 epochs each) ===\")\n",
        "print(\"+Attention :\", m_attn)\n",
        "print(\"NoAttention:\", m_noatt)\n",
        "\n",
        "# bar chart\n",
        "metrics = [\"AUC\", \"F1\", \"ACC\", \"SEN\", \"SPE\"]\n",
        "attn_vals = [m_attn[k] for k in metrics]\n",
        "noatt_vals = [m_noatt[k] for k in metrics]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(7, 3))\n",
        "x = np.arange(len(metrics))\n",
        "w = 0.35\n",
        "plt.bar(x - w / 2, attn_vals, width=w, label=\"+Attention (5ep)\")\n",
        "plt.bar(x + w / 2, noatt_vals, width=w, label=\"Baseline (5ep)\")\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylim(0.80, 1.00)\n",
        "for i, v in enumerate(attn_vals):\n",
        "    plt.text(i - w / 2, v + 0.005, f\"{v:.3f}\", ha=\"center\", fontsize=8)\n",
        "for i, v in enumerate(noatt_vals):\n",
        "    plt.text(i + w / 2, v + 0.005, f\"{v:.3f}\", ha=\"center\", fontsize=8)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Fair Comparison on TEST (5 epochs each)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "fig_path = \"/content/compare_equal.png\"\n",
        "plt.savefig(fig_path, dpi=180)\n",
        "plt.close()\n",
        "print(\"Saved figure:\", fig_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wOqOjIUvVXx",
        "outputId": "e3c3c69a-3f6f-4396-a084-21c7fee33219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TEST (5 epochs each) ===\n",
            "+Attention : {'AUC': 0.9733249999999999, 'F1': 0.9247842170160296, 'ACC': 0.92375, 'SEN': 0.9374999999765625, 'SPE': 0.90999999997725}\n",
            "NoAttention: {'AUC': 0.970375, 'F1': 0.929471032745592, 'ACC': 0.93, 'SEN': 0.9224999999769374, 'SPE': 0.9374999999765625}\n",
            "Saved figure: /content/compare_equal.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# G1) Grad-CAM for +Attention (5ep) on a few TEST images\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_ROOT = \"/content/data\"\n",
        "CKPT = \"/content/runs/attn_5ep.pt\"\n",
        "OUTDIR = \"/content/gradcam_equal\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# data\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "ds = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm)\n",
        "\n",
        "# model\n",
        "model = build_model(pretrained=False, use_attn=True).to(DEVICE).eval()\n",
        "state = torch.load(CKPT, map_location=DEVICE, weights_only=False)\n",
        "model.load_state_dict(state[\"model\"])\n",
        "\n",
        "# last conv for Grad-CAM\n",
        "target_module = model.backbone.layer4[-1].conv2\n",
        "feats, grads = [], []\n",
        "h1 = target_module.register_forward_hook(lambda m, i, o: feats.append(o.detach()))\n",
        "h2 = target_module.register_full_backward_hook(\n",
        "    lambda m, gin, gout: grads.append(gout[0].detach())\n",
        ")\n",
        "\n",
        "\n",
        "def gradcam(img_t):\n",
        "    feats.clear()\n",
        "    grads.clear()\n",
        "    img_t = img_t.unsqueeze(0).to(DEVICE)\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    logits = model(img_t)\n",
        "    prob = torch.sigmoid(logits)[0]\n",
        "    prob.backward(retain_graph=True)\n",
        "    A = feats[-1][0]  # [C,H,W]\n",
        "    G = grads[-1][0]  # [C,H,W]\n",
        "    w = G.mean(dim=(1, 2), keepdim=True)\n",
        "    cam = (A * w).sum(dim=0).cpu().numpy()\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / (cam.max() + 1e-8)\n",
        "    return float(prob.detach().cpu()), cam\n",
        "\n",
        "\n",
        "def read_raw(path):\n",
        "    return np.array(Image.open(path).convert(\"L\").resize((512, 512)))\n",
        "\n",
        "\n",
        "# مسیرها طبق ترتیب داخلی دیتاست\n",
        "paths = [p for p, _ in ds.samples]\n",
        "\n",
        "# انتخاب: 2×TP, 2×TN, 1×FP, 1×FN (با اطمینان بالا)\n",
        "samples = {\"TP\": [], \"TN\": [], \"FP\": [], \"FN\": []}\n",
        "with torch.no_grad():\n",
        "    for idx in range(len(ds)):\n",
        "        x, y = ds[idx]\n",
        "        p = torch.sigmoid(model(x.unsqueeze(0).to(DEVICE)))[0].item()\n",
        "        pred = 1.0 if p >= 0.5 else 0.0\n",
        "        key = (\n",
        "            \"TP\"\n",
        "            if (pred == 1 and y.item() == 1)\n",
        "            else \"TN\"\n",
        "            if (pred == 0 and y.item() == 0)\n",
        "            else \"FP\"\n",
        "            if (pred == 1 and y.item() == 0)\n",
        "            else \"FN\"\n",
        "        )\n",
        "        samples[key].append((idx, float(p)))\n",
        "\n",
        "samples[\"TP\"] = sorted(samples[\"TP\"], key=lambda t: -t[1])[:2]\n",
        "samples[\"TN\"] = sorted(samples[\"TN\"], key=lambda t: t[1])[:2]\n",
        "samples[\"FP\"] = sorted(samples[\"FP\"], key=lambda t: -t[1])[:1]\n",
        "samples[\"FN\"] = sorted(samples[\"FN\"], key=lambda t: t[1])[:1]\n",
        "\n",
        "count = 0\n",
        "for k in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
        "    for idx, _ in samples[k]:\n",
        "        x, y = ds[idx]\n",
        "        prob, cam = gradcam(x)\n",
        "        raw = read_raw(paths[idx])\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        plt.imshow(raw, cmap=\"gray\")\n",
        "        plt.imshow(cam, alpha=0.35)\n",
        "        title = f\"{k} | prob={prob:.2f} | true={int(y.item())} | pred={int(prob>=0.5)}\"\n",
        "        plt.title(title)\n",
        "        plt.axis(\"off\")\n",
        "        fname = os.path.join(OUTDIR, f\"{k.lower()}_{idx:05d}.png\")\n",
        "        plt.savefig(fname, dpi=160, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        print(\"Saved:\", fname, \"::\", title)\n",
        "        count += 1\n",
        "\n",
        "h1.remove()\n",
        "h2.remove()\n",
        "print(f\"\\nTotal saved: {count} | Folder: {OUTDIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6eJG83lzwfA",
        "outputId": "e434e2a6-b0c2-4e7e-9e80-29aa2afa95d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/gradcam_equal/tp_00573.png :: TP | prob=1.00 | true=1 | pred=1\n",
            "Saved: /content/gradcam_equal/tp_00684.png :: TP | prob=1.00 | true=1 | pred=1\n",
            "Saved: /content/gradcam_equal/tn_00252.png :: TN | prob=0.00 | true=0 | pred=0\n",
            "Saved: /content/gradcam_equal/tn_00369.png :: TN | prob=0.00 | true=0 | pred=0\n",
            "Saved: /content/gradcam_equal/fp_00358.png :: FP | prob=0.98 | true=0 | pred=1\n",
            "Saved: /content/gradcam_equal/fn_00538.png :: FN | prob=0.00 | true=1 | pred=0\n",
            "\n",
            "Total saved: 6 | Folder: /content/gradcam_equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A1) Train +Attention up to 15 epochs with Early-Stopping (patience=3)\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_ROOT = \"/content/data\"\n",
        "EPOCHS = 15\n",
        "PATIENCE = 3\n",
        "\n",
        "tfm_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "tfm_eval = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "ds_train = MammogramFolders(f\"{DATA_ROOT}/train\", transform=tfm_train)\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm_eval)\n",
        "dl_train = DataLoader(\n",
        "    ds_train, batch_size=16, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=32, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "model = build_model(pretrained=True, use_attn=True).to(DEVICE)\n",
        "pos_weight = torch.tensor([ds_train.pos_weight()], device=DEVICE)\n",
        "crit = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
        "scaler = GradScaler(device=DEVICE if DEVICE == \"cuda\" else \"cpu\")\n",
        "\n",
        "\n",
        "def val_auc(m, loader):\n",
        "    m.eval()\n",
        "    Ys, Ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            p = torch.sigmoid(m(x.to(DEVICE)))\n",
        "            Ys.append(y.float().numpy())\n",
        "            Ps.append(p.float().cpu().numpy())\n",
        "    y = np.concatenate(Ys)\n",
        "    p = np.concatenate(Ps)\n",
        "    return float(roc_auc_score(y, p))\n",
        "\n",
        "\n",
        "hist = {\"loss\": [], \"auc\": []}\n",
        "best_auc, best_ep, patience = -1, -1, 0\n",
        "os.makedirs(\"/content/runs\", exist_ok=True)\n",
        "CKPT = \"/content/runs/attn_early.pt\"\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    run = 0\n",
        "    n = 0\n",
        "    for x, y in dl_train:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "            loss = crit(model(x), y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        run += loss.item() * x.size(0)\n",
        "        n += x.size(0)\n",
        "    sch.step()\n",
        "    L = run / max(1, n)\n",
        "    A = val_auc(model, dl_val)\n",
        "    hist[\"loss\"].append(L)\n",
        "    hist[\"auc\"].append(A)\n",
        "    print(f\"[Ep {ep}/{EPOCHS}] loss={L:.4f} | val AUC={A:.3f} | {time.time()-t0:.1f}s\")\n",
        "    if A > best_auc:\n",
        "        best_auc, best_ep, patience = A, ep, 0\n",
        "        torch.save({\"model\": model.state_dict(), \"best_ep\": ep, \"val_auc\": A}, CKPT)\n",
        "        print(\"  ✅ Saved best:\", CKPT)\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= PATIENCE:\n",
        "            print(f\"⏹ Early-stopped at epoch {ep} (best={best_ep})\")\n",
        "            break\n",
        "\n",
        "# curves\n",
        "plt.figure(figsize=(6, 3))\n",
        "ax1 = plt.gca()\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(hist[\"loss\"], label=\"Train Loss\")\n",
        "ax2.plot(hist[\"auc\"], label=\"Val AUC\", color=\"tab:orange\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax2.set_ylabel(\"AUC\")\n",
        "ln1, lb1 = ax1.get_legend_handles_labels()\n",
        "ln2, lb2 = ax2.get_legend_handles_labels()\n",
        "plt.legend(ln1 + ln2, lb1 + lb2, loc=\"center right\")\n",
        "plt.title(\"Curves (+Attention, EarlyStopping)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/curves_attn15.png\", dpi=180)\n",
        "plt.close()\n",
        "print(\"Saved curves: /content/curves_attn15.png\")\n",
        "print(f\"Best val AUC={best_auc:.3f} at epoch {best_ep} | Checkpoint={CKPT}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5co8Aibk3HK_",
        "outputId": "eefb02fb-7d8d-471a-8266-265ea48c6ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ep 1/15] loss=0.5192 | val AUC=0.926 | 162.3s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 2/15] loss=0.3281 | val AUC=0.960 | 156.9s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 3/15] loss=0.2743 | val AUC=0.954 | 153.7s\n",
            "[Ep 4/15] loss=0.2410 | val AUC=0.975 | 158.9s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 5/15] loss=0.2053 | val AUC=0.975 | 155.5s\n",
            "[Ep 6/15] loss=0.1958 | val AUC=0.976 | 154.0s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 7/15] loss=0.1875 | val AUC=0.973 | 155.1s\n",
            "[Ep 8/15] loss=0.1480 | val AUC=0.977 | 159.5s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 9/15] loss=0.1400 | val AUC=0.978 | 155.3s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 10/15] loss=0.1276 | val AUC=0.982 | 155.5s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 11/15] loss=0.1091 | val AUC=0.983 | 152.6s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 12/15] loss=0.0953 | val AUC=0.984 | 156.6s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 13/15] loss=0.0925 | val AUC=0.985 | 157.8s\n",
            "  ✅ Saved best: /content/runs/attn_early.pt\n",
            "[Ep 14/15] loss=0.0859 | val AUC=0.984 | 152.6s\n",
            "[Ep 15/15] loss=0.0851 | val AUC=0.984 | 157.2s\n",
            "Saved curves: /content/curves_attn15.png\n",
            "Best val AUC=0.985 at epoch 13 | Checkpoint=/content/runs/attn_early.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A2) Pick operating thresholds from VAL, then evaluate on TEST\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from dataset import MammogramFolders\n",
        "from model import build_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_ROOT = \"/content/data\"\n",
        "CKPT = \"/content/runs/attn_early.pt\"\n",
        "\n",
        "# --- data ---\n",
        "tfm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "ds_val = MammogramFolders(f\"{DATA_ROOT}/val\", transform=tfm)\n",
        "ds_test = MammogramFolders(f\"{DATA_ROOT}/test\", transform=tfm)\n",
        "dl_val = DataLoader(\n",
        "    ds_val, batch_size=64, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "dl_test = DataLoader(\n",
        "    ds_test, batch_size=64, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# --- model ---\n",
        "state = torch.load(CKPT, map_location=DEVICE, weights_only=False)\n",
        "model = build_model(pretrained=False, use_attn=True).to(DEVICE).eval()\n",
        "model.load_state_dict(state[\"model\"])\n",
        "\n",
        "\n",
        "def collect_probs(loader):\n",
        "    Ys, Ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            p = torch.sigmoid(model(x.to(DEVICE))).float().cpu().numpy()\n",
        "            Ys.append(y.float().cpu().numpy())\n",
        "            Ps.append(p)\n",
        "    y = np.concatenate(Ys)\n",
        "    p = np.concatenate(Ps)\n",
        "    return y, p\n",
        "\n",
        "\n",
        "y_val, p_val = collect_probs(dl_val)\n",
        "y_test, p_test = collect_probs(dl_test)\n",
        "\n",
        "# --- thresholds from VAL ---\n",
        "fpr, tpr, thr = roc_curve(y_val, p_val)\n",
        "auc_val = roc_auc_score(y_val, p_val)\n",
        "\n",
        "# 1) default\n",
        "t_default = 0.5\n",
        "# 2) Youden's J\n",
        "j = tpr - fpr\n",
        "t_j = float(thr[np.argmax(j)])\n",
        "# 3) target SEN ≈ 0.95  (closest)\n",
        "target_sen = 0.95\n",
        "idx_sen = np.argmin(np.abs(tpr - target_sen))\n",
        "t_sen95 = float(thr[idx_sen])\n",
        "\n",
        "\n",
        "def metrics_at_threshold(y_true, y_prob, t):\n",
        "    y_pred = (y_prob >= t).astype(np.float32)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sen = tp / (tp + fn + 1e-8)\n",
        "    spe = tn / (tn + fp + 1e-8)\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    return dict(\n",
        "        TH=t,\n",
        "        AUC=float(auc),\n",
        "        F1=float(f1),\n",
        "        ACC=float(acc),\n",
        "        SEN=float(sen),\n",
        "        SPE=float(spe),\n",
        "        TP=int(tp),\n",
        "        TN=int(tn),\n",
        "        FP=int(fp),\n",
        "        FN=int(fn),\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"VAL AUC={auc_val:.3f}\")\n",
        "Ts = [(\"default_0.50\", t_default), (\"youdenJ\", t_j), (\"sen≈0.95\", t_sen95)]\n",
        "\n",
        "# --- evaluate on TEST for each threshold ---\n",
        "results = {}\n",
        "for name, tt in Ts:\n",
        "    results[name] = metrics_at_threshold(y_test, p_test, tt)\n",
        "    print(name, \"->\", results[name])\n",
        "\n",
        "# --- plot ROC with operating points (from VAL) ---\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(fpr, tpr, label=f\"Val ROC (AUC={auc_val:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], \"--\", alpha=0.5)\n",
        "\n",
        "\n",
        "# mark points\n",
        "def point_for_t(t):\n",
        "    # find nearest threshold index for plotting\n",
        "    i = np.argmin(np.abs(thr - t))\n",
        "    return fpr[i], tpr[i]\n",
        "\n",
        "\n",
        "for name, tt in Ts:\n",
        "    x, y = point_for_t(tt)\n",
        "    plt.scatter([x], [y], label=f\"{name} (t={tt:.2f})\")\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.title(\"VAL ROC with chosen thresholds\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "out_fig = \"/content/roc_op_points.png\"\n",
        "plt.savefig(out_fig, dpi=180)\n",
        "plt.close()\n",
        "print(\"Saved figure:\", out_fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W28zSeUfG7Wx",
        "outputId": "ff2ff34c-cd89-4c27-9445-326a9697c0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL AUC=0.985\n",
            "default_0.50 -> {'TH': 0.5, 'AUC': 0.98006875, 'F1': 0.9432098765432099, 'ACC': 0.9424999999882188, 'SEN': 0.954999999976125, 'SPE': 0.92999999997675, 'TP': 382, 'TN': 372, 'FP': 28, 'FN': 18}\n",
            "youdenJ -> {'TH': 0.6269708871841431, 'AUC': 0.98006875, 'F1': 0.9389788293897883, 'ACC': 0.9387499999882656, 'SEN': 0.9424999999764375, 'SPE': 0.934999999976625, 'TP': 377, 'TN': 374, 'FP': 26, 'FN': 23}\n",
            "sen≈0.95 -> {'TH': 0.7321080565452576, 'AUC': 0.98006875, 'F1': 0.9423558897243107, 'ACC': 0.9424999999882188, 'SEN': 0.9399999999765, 'SPE': 0.944999999976375, 'TP': 376, 'TN': 378, 'FP': 22, 'FN': 24}\n",
            "Saved figure: /content/roc_op_points.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# A3) Pretty table + Confusion Matrices for the three thresholds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# نتایج مرحله قبل را اینجا کپی می‌کنیم (عیناً از لاگ شما)\n",
        "res = {\n",
        "    \"default_0.50\": {\n",
        "        \"TH\": 0.5,\n",
        "        \"AUC\": 0.98006875,\n",
        "        \"F1\": 0.9432098765432099,\n",
        "        \"ACC\": 0.9424999999882188,\n",
        "        \"SEN\": 0.954999999976125,\n",
        "        \"SPE\": 0.92999999997675,\n",
        "        \"TP\": 382,\n",
        "        \"TN\": 372,\n",
        "        \"FP\": 28,\n",
        "        \"FN\": 18,\n",
        "    },\n",
        "    \"youdenJ\": {\n",
        "        \"TH\": 0.6269708871841431,\n",
        "        \"AUC\": 0.98006875,\n",
        "        \"F1\": 0.9389788293897883,\n",
        "        \"ACC\": 0.9387499999882656,\n",
        "        \"SEN\": 0.9424999999764375,\n",
        "        \"SPE\": 0.934999999976625,\n",
        "        \"TP\": 377,\n",
        "        \"TN\": 374,\n",
        "        \"FP\": 26,\n",
        "        \"FN\": 23,\n",
        "    },\n",
        "    \"sen≈0.95\": {\n",
        "        \"TH\": 0.7321080565452576,\n",
        "        \"AUC\": 0.98006875,\n",
        "        \"F1\": 0.9423558897243107,\n",
        "        \"ACC\": 0.9424999999882188,\n",
        "        \"SEN\": 0.9399999999765,\n",
        "        \"SPE\": 0.944999999976375,\n",
        "        \"TP\": 376,\n",
        "        \"TN\": 378,\n",
        "        \"FP\": 22,\n",
        "        \"FN\": 24,\n",
        "    },\n",
        "}\n",
        "\n",
        "# 1) جدول تمیز\n",
        "df = pd.DataFrame(res).T[\n",
        "    [\"TH\", \"AUC\", \"F1\", \"ACC\", \"SEN\", \"SPE\", \"TP\", \"TN\", \"FP\", \"FN\"]\n",
        "]\n",
        "df = df.rename(\n",
        "    index={\n",
        "        \"default_0.50\": \"default(0.50)\",\n",
        "        \"youdenJ\": \"youdenJ(~0.63)\",\n",
        "        \"sen≈0.95\": \"sen≈0.95(~0.73)\",\n",
        "    }\n",
        ")\n",
        "print(df.round(3))\n",
        "df.to_csv(\"/content/threshold_comparison.csv\", index=True)\n",
        "print(\"Saved CSV -> /content/threshold_comparison.csv\")\n",
        "\n",
        "\n",
        "# 2) CM figure helper\n",
        "def save_cm(tp, tn, fp, fn, title, path):\n",
        "    mat = np.array([[tn, fp], [fn, tp]])\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(mat, cmap=\"viridis\")\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(\n",
        "                j, i, str(mat[i, j]), ha=\"center\", va=\"center\", color=\"w\", fontsize=14\n",
        "            )\n",
        "    plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"])\n",
        "    plt.yticks([0, 1], [\"True 0\", \"True 1\"])\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=180)\n",
        "    plt.close()\n",
        "    print(\"Saved:\", path)\n",
        "\n",
        "\n",
        "os.makedirs(\"/content\", exist_ok=True)\n",
        "save_cm(\n",
        "    **{\n",
        "        \"tp\": res[\"default_0.50\"][\"TP\"],\n",
        "        \"tn\": res[\"default_0.50\"][\"TN\"],\n",
        "        \"fp\": res[\"default_0.50\"][\"FP\"],\n",
        "        \"fn\": res[\"default_0.50\"][\"FN\"],\n",
        "    },\n",
        "    title=\"Confusion Matrix – default (t=0.50)\",\n",
        "    path=\"/content/cm_default.png\",\n",
        ")\n",
        "\n",
        "save_cm(\n",
        "    **{\n",
        "        \"tp\": res[\"youdenJ\"][\"TP\"],\n",
        "        \"tn\": res[\"youdenJ\"][\"TN\"],\n",
        "        \"fp\": res[\"youdenJ\"][\"FP\"],\n",
        "        \"fn\": res[\"youdenJ\"][\"FN\"],\n",
        "    },\n",
        "    title=\"Confusion Matrix – YoudenJ (~0.63)\",\n",
        "    path=\"/content/cm_youden.png\",\n",
        ")\n",
        "\n",
        "save_cm(\n",
        "    **{\n",
        "        \"tp\": res[\"sen≈0.95\"][\"TP\"],\n",
        "        \"tn\": res[\"sen≈0.95\"][\"TN\"],\n",
        "        \"fp\": res[\"sen≈0.95\"][\"FP\"],\n",
        "        \"fn\": res[\"sen≈0.95\"][\"FN\"],\n",
        "    },\n",
        "    title=\"Confusion Matrix – SEN≈0.95 (~0.73)\",\n",
        "    path=\"/content/cm_sen95.png\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYLoPk1_IdT7",
        "outputId": "ae8271c2-e903-4267-855a-90f3d7f299fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    TH   AUC     F1    ACC    SEN    SPE     TP     TN    FP  \\\n",
            "default(0.50)    0.500  0.98  0.943  0.942  0.955  0.930  382.0  372.0  28.0   \n",
            "youdenJ(~0.63)   0.627  0.98  0.939  0.939  0.942  0.935  377.0  374.0  26.0   \n",
            "sen≈0.95(~0.73)  0.732  0.98  0.942  0.942  0.940  0.945  376.0  378.0  22.0   \n",
            "\n",
            "                   FN  \n",
            "default(0.50)    18.0  \n",
            "youdenJ(~0.63)   23.0  \n",
            "sen≈0.95(~0.73)  24.0  \n",
            "Saved CSV -> /content/threshold_comparison.csv\n",
            "Saved: /content/cm_default.png\n",
            "Saved: /content/cm_youden.png\n",
            "Saved: /content/cm_sen95.png\n"
          ]
        }
      ]
    }
  ]
}